{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9759296,"sourceType":"datasetVersion","datasetId":5976018},{"sourceId":9906621,"sourceType":"datasetVersion","datasetId":6086394},{"sourceId":9908616,"sourceType":"datasetVersion","datasetId":6087896},{"sourceId":9912524,"sourceType":"datasetVersion","datasetId":6090865},{"sourceId":10299994,"sourceType":"datasetVersion","datasetId":6375223},{"sourceId":10748629,"sourceType":"datasetVersion","datasetId":5975530},{"sourceId":11100474,"sourceType":"datasetVersion","datasetId":6909527},{"sourceId":11144025,"sourceType":"datasetVersion","datasetId":6951699},{"sourceId":11152907,"sourceType":"datasetVersion","datasetId":6664883}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hello Researchers, in this notebook, we have performed various downstream tasks, such as viewing the maps of different upsampled GWL, analyzing feature importance, and examining SHAP values of the upsampled model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport joblib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-03-28T09:01:02.858751Z","iopub.execute_input":"2025-03-28T09:01:02.859168Z","iopub.status.idle":"2025-03-28T09:01:03.321763Z","shell.execute_reply.started":"2025-03-28T09:01:02.859098Z","shell.execute_reply":"2025-03-28T09:01:03.320513Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q openpyxl\n!pip install -q xlrd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:03.324023Z","iopub.execute_input":"2025-03-28T09:01:03.324534Z","iopub.status.idle":"2025-03-28T09:01:26.190850Z","shell.execute_reply.started":"2025-03-28T09:01:03.324497Z","shell.execute_reply":"2025-03-28T09:01:26.189053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the low resolution data\n### These are the GLDAS data and their vegetation indices.","metadata":{}},{"cell_type":"code","source":"gldas_veg_index=pd.read_excel('/kaggle/input/new-data/GLDAS1_Gridwise_ndvi_ndwi_14Feb2025.xls')\ngldas=pd.read_excel('/kaggle/input/new-data/GLDAS1_Gridwise_14HgfValue_flood_clay_and GWL 13Feb2025.xls')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.192596Z","iopub.execute_input":"2025-03-28T09:01:26.192970Z","iopub.status.idle":"2025-03-28T09:01:26.318720Z","shell.execute_reply.started":"2025-03-28T09:01:26.192934Z","shell.execute_reply":"2025-03-28T09:01:26.317519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gldas = gldas.merge(gldas_veg_index, on='SerialID', how='inner').rename(columns={'SerialID':'GLDAS_SerialID','X':'POINT_X','Y':'POINT_Y'})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.320163Z","iopub.execute_input":"2025-03-28T09:01:26.320577Z","iopub.status.idle":"2025-03-28T09:01:26.340876Z","shell.execute_reply.started":"2025-03-28T09:01:26.320540Z","shell.execute_reply":"2025-03-28T09:01:26.339913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"low_res = pd.read_csv('/kaggle/input/test-gldas-data/gldas_data_yearwise.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.343338Z","iopub.execute_input":"2025-03-28T09:01:26.343696Z","iopub.status.idle":"2025-03-28T09:01:26.380170Z","shell.execute_reply.started":"2025-03-28T09:01:26.343661Z","shell.execute_reply":"2025-03-28T09:01:26.378870Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Function to split the years","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef reshape_by_year(df):\n    \"\"\"\n    Reshape the input dataframe so that each row corresponds to a specific year with \n    NDVI, NDWI, Min-GWL, and Max-GWL values for that year.\n    \n    Parameters:\n    df (pd.DataFrame): Original dataframe containing the yearly data in separate columns.\n    \n    Returns:\n    pd.DataFrame: Reshaped dataframe with one row per year containing NDVI, NDWI, Min-GWL, and Max-GWL.\n    \"\"\"\n    # Step 1: Create a list of years\n    years = list(range(2003, 2025))  # Assuming the years range from 2001 to 2023\n\n    # Step 2: Prepare a dataframe for each year with NDWI, NDVI, and Min/Max GWL\n    reshaped_df = pd.DataFrame()\n#     print(df.shape)\n    for year in years:\n        # Filter the columns related to the current year\n        year_columns = [f'NDVI{year}', f'NDWI{year}', f'Min-gwl_{year}', f'Max-gwl_{year}']\n        \n        # Create a temporary dataframe for the current year\n        year_df = df[low_res.drop(columns=['Unnamed: 0','NDVI', 'NDWI', 'Min_GWS',\n       'Max_GWS', 'Year']).columns.tolist()].copy()\n\n        # Add the NDVI, NDWI, and GWL columns for the current year\n        year_df['NDVI'] = df[f'ndvi{year}']\n        year_df['NDWI'] = df[f'ndwi{year}']\n        year_df['Min_GWS'] = df[f'Min_GWS_{year}']\n        year_df['Max_GWS'] = df[f'Max_GWS_{year}']\n        \n        # Add the year as a column\n        year_df['Year'] = year\n#         print(reshaped_df.shape)\n        # Append the temporary dataframe to the final dataframe\n        reshaped_df = pd.concat([reshaped_df, year_df], ignore_index=True)\n\n    # Step 3: Return the reshaped dataframe\n    return reshaped_df\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.381516Z","iopub.execute_input":"2025-03-28T09:01:26.381839Z","iopub.status.idle":"2025-03-28T09:01:26.390476Z","shell.execute_reply.started":"2025-03-28T09:01:26.381809Z","shell.execute_reply":"2025-03-28T09:01:26.389221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_low=reshape_by_year(gldas[low_res.drop(columns=['Unnamed: 0','NDVI', 'NDWI', 'Min_GWS',\n       'Max_GWS', 'Year']).columns.tolist()+\ngldas.columns[gldas.columns.str.startswith('nd')].tolist()\n+ gldas.columns[gldas.columns.str.contains('GWS')].tolist()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.391812Z","iopub.execute_input":"2025-03-28T09:01:26.392140Z","iopub.status.idle":"2025-03-28T09:01:26.467223Z","shell.execute_reply.started":"2025-03-28T09:01:26.392089Z","shell.execute_reply":"2025-03-28T09:01:26.466068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_low.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.468542Z","iopub.execute_input":"2025-03-28T09:01:26.468864Z","iopub.status.idle":"2025-03-28T09:01:26.476864Z","shell.execute_reply.started":"2025-03-28T09:01:26.468833Z","shell.execute_reply":"2025-03-28T09:01:26.475809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for i in gldas.columns:\n#     print(i)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.478208Z","iopub.execute_input":"2025-03-28T09:01:26.478527Z","iopub.status.idle":"2025-03-28T09:01:26.483054Z","shell.execute_reply.started":"2025-03-28T09:01:26.478497Z","shell.execute_reply":"2025-03-28T09:01:26.481905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef reshape_by_year(df):\n    \"\"\"\n    Reshape the input dataframe so that each row corresponds to a specific year with \n    NDVI, NDWI, Min-GWL, and Max-GWL values for that year.\n    \n    Parameters:\n    df (pd.DataFrame): Original dataframe containing the yearly data in separate columns.\n    \n    Returns:\n    pd.DataFrame: Reshaped dataframe with one row per year containing NDVI, NDWI, Min-GWL, and Max-GWL.\n    \"\"\"\n    # Step 1: Create a list of years\n    years = list(range(2003, 2025))  # Assuming the years range from 2001 to 2023\n\n    # Step 2: Prepare a dataframe for each year with NDWI, NDVI, and Min/Max GWL\n    reshaped_df = pd.DataFrame()\n#     print(df.shape)\n    for year in years:\n        # Filter the columns related to the current year\n        year_columns = [f'NDVI{year}', f'NDWI{year}', f'Min-gwl_{year}', f'Max-gwl_{year}']\n        \n        # Create a temporary dataframe for the current year\n        # year_df = df[[ 'POINT_X', 'POINT_Y', \n        #                'TWI', 'TRI', 'Sy', 'STI', 'SPI', 'Slope', \n        #               'Profile_curvature', 'Plan_curvature', 'lithology', 'lithology_clay_thickness', \n        #               'Distance_from_stream', 'elevation', 'drainage_density', 'Curvature', \n        #               'Aspect', 'GLDAS_SerialID']].copy()\n        year_df = df\n        # Add the NDVI, NDWI, and GWL columns for the current year\n        year_df['NDVI'] = df[f'NDVI{year}']\n        year_df['NDWI'] = df[f'NDWI{year}']\n        # year_df['Min_GWL'] = df[f'Min-gwl_{year}']\n        # year_df['Max_GWL'] = df[f'Max-gwl_{year}']\n        \n        # Add the year as a column\n        year_df['Year'] = year\n#         print(reshaped_df.shape)\n        # Append the temporary dataframe to the final dataframe\n        reshaped_df = pd.concat([reshaped_df, year_df], ignore_index=True)\n\n    # Step 3: Return the reshaped dataframe\n    return reshaped_df\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.484548Z","iopub.execute_input":"2025-03-28T09:01:26.484900Z","iopub.status.idle":"2025-03-28T09:01:26.492554Z","shell.execute_reply.started":"2025-03-28T09:01:26.484868Z","shell.execute_reply":"2025-03-28T09:01:26.491354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2km uniform grid center points\n","metadata":{}},{"cell_type":"code","source":"new_high_res=pd.read_excel(\"/kaggle/input/new-data/2k_points_14Hgfs_ndvi_ndwi_14Feb2025.xls\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:26.494178Z","iopub.execute_input":"2025-03-28T09:01:26.494642Z","iopub.status.idle":"2025-03-28T09:01:32.532502Z","shell.execute_reply.started":"2025-03-28T09:01:26.494593Z","shell.execute_reply":"2025-03-28T09:01:32.531459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Points where people usually take measurements\n\n#### In these location various measurements have been taken by various organizations","metadata":{}},{"cell_type":"code","source":"import pandas as pd\noriginal=pd.read_csv('/kaggle/input/new-data/new_insitu_points.csv',low_memory=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:32.533769Z","iopub.execute_input":"2025-03-28T09:01:32.534105Z","iopub.status.idle":"2025-03-28T09:01:32.929375Z","shell.execute_reply.started":"2025-03-28T09:01:32.534073Z","shell.execute_reply":"2025-03-28T09:01:32.928366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:32.931989Z","iopub.execute_input":"2025-03-28T09:01:32.932356Z","iopub.status.idle":"2025-03-28T09:01:32.939324Z","shell.execute_reply.started":"2025-03-28T09:01:32.932321Z","shell.execute_reply":"2025-03-28T09:01:32.938218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GLDAS ID for the 2km uniform points","metadata":{}},{"cell_type":"code","source":"high_gldas_id=pd.read_excel('/kaggle/input/new-data/2kpoints_gldas_grid 19Sept2024 (1).xls').rename(columns={'2k_Points_UniqID':'UniqID'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:32.944888Z","iopub.execute_input":"2025-03-28T09:01:32.945352Z","iopub.status.idle":"2025-03-28T09:01:33.503269Z","shell.execute_reply.started":"2025-03-28T09:01:32.945307Z","shell.execute_reply":"2025-03-28T09:01:33.501527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_high_res=new_high_res.merge(high_gldas_id,on=['UniqID'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:33.505212Z","iopub.execute_input":"2025-03-28T09:01:33.505709Z","iopub.status.idle":"2025-03-28T09:01:33.539572Z","shell.execute_reply.started":"2025-03-28T09:01:33.505637Z","shell.execute_reply":"2025-03-28T09:01:33.538220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\nnew_high_res.columns = [re.sub(r'(ndvi|ndwi)(\\d{2})$', lambda m: f\"{m.group(1).upper()}20{m.group(2)}\", col) for col in new_high_res.columns]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:33.541312Z","iopub.execute_input":"2025-03-28T09:01:33.541719Z","iopub.status.idle":"2025-03-28T09:01:33.547717Z","shell.execute_reply.started":"2025-03-28T09:01:33.541682Z","shell.execute_reply":"2025-03-28T09:01:33.546483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_high_res.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:33.549362Z","iopub.execute_input":"2025-03-28T09:01:33.549843Z","iopub.status.idle":"2025-03-28T09:01:33.557419Z","shell.execute_reply.started":"2025-03-28T09:01:33.549798Z","shell.execute_reply":"2025-03-28T09:01:33.556181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imputer_col=['lithology_clay_thickness', 'TWI', 'TRI', 'Sy', 'STI', 'SPI', 'Slope', \n'Profile_curvature', 'Plan_curvature', 'Distance_from_stream', 'elevation', \n'drainage_density', 'Curvature', 'Aspect', 'lithology', 'NDVI2001', 'NDVI2002',\n'NDVI2003', 'NDVI2004', 'NDVI2005', 'NDVI2006', 'NDVI2007', 'NDVI2008', 'NDVI2009',\n'NDVI2010', 'NDVI2011', 'NDVI2012', 'NDVI2013', 'NDVI2014', 'NDVI2015', 'NDVI2016', \n'NDVI2017', 'NDVI2018', 'NDVI2019', 'NDVI2020', 'NDVI2021', 'NDVI2022', 'NDVI2023', \n'NDWI2001', 'NDWI2002', 'NDWI2003', 'NDWI2004', 'NDWI2005', 'NDWI2006', 'NDWI2007', \n'NDWI2008', 'NDWI2009', 'NDWI2010', 'NDWI2011', 'NDWI2012', 'NDWI2013', 'NDWI2014',\n'NDWI2015', 'NDWI2016', 'NDWI2017', 'NDWI2018', 'NDWI2019', 'NDWI2020', 'NDWI2021', \n'NDWI2022', 'NDWI2023']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:33.558958Z","iopub.execute_input":"2025-03-28T09:01:33.559417Z","iopub.status.idle":"2025-03-28T09:01:33.566605Z","shell.execute_reply.started":"2025-03-28T09:01:33.559370Z","shell.execute_reply":"2025-03-28T09:01:33.565383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputing the missing values","metadata":{}},{"cell_type":"code","source":"imputer=joblib.load('/kaggle/input/test-wights-for-groundhog/imputer.pkl')\nnew_high_res[imputer_col]=imputer.transform(new_high_res[imputer_col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:33.567965Z","iopub.execute_input":"2025-03-28T09:01:33.568307Z","iopub.status.idle":"2025-03-28T09:01:44.397489Z","shell.execute_reply.started":"2025-03-28T09:01:33.568276Z","shell.execute_reply":"2025-03-28T09:01:44.396261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"imputer=joblib.load('/kaggle/input/test-wights-for-groundhog/imputer.pkl')\noriginal[imputer_col]=imputer.transform(original[imputer_col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:44.398959Z","iopub.execute_input":"2025-03-28T09:01:44.399495Z","iopub.status.idle":"2025-03-28T09:01:44.743284Z","shell.execute_reply.started":"2025-03-28T09:01:44.399456Z","shell.execute_reply":"2025-03-28T09:01:44.742150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_high_res.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:44.744773Z","iopub.execute_input":"2025-03-28T09:01:44.745086Z","iopub.status.idle":"2025-03-28T09:01:44.752356Z","shell.execute_reply.started":"2025-03-28T09:01:44.745055Z","shell.execute_reply":"2025-03-28T09:01:44.751284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reshaping for easier use case","metadata":{}},{"cell_type":"code","source":"new_high=reshape_by_year(new_high_res)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:44.753681Z","iopub.execute_input":"2025-03-28T09:01:44.754004Z","iopub.status.idle":"2025-03-28T09:01:46.353791Z","shell.execute_reply.started":"2025-03-28T09:01:44.753972Z","shell.execute_reply":"2025-03-28T09:01:46.352251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original=reshape_by_year(original)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:46.356315Z","iopub.execute_input":"2025-03-28T09:01:46.356730Z","iopub.status.idle":"2025-03-28T09:01:47.203095Z","shell.execute_reply.started":"2025-03-28T09:01:46.356690Z","shell.execute_reply":"2025-03-28T09:01:47.201929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_high.isnull().any()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:47.204543Z","iopub.execute_input":"2025-03-28T09:01:47.204911Z","iopub.status.idle":"2025-03-28T09:01:47.265822Z","shell.execute_reply.started":"2025-03-28T09:01:47.204877Z","shell.execute_reply":"2025-03-28T09:01:47.264558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Trigger Download","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink, display, HTML, Javascript\n\ndef trigger_download(filename, my_id=0):\n    # Create the FileLink object\n    file_link = FileLink(filename)\n\n    # Get the path of the file\n    file_path = file_link.path\n\n    # Create the HTML link\n    html = f'<a id=\"download_link_{file_path}_{my_id}\" href=\"{file_path}\" download>{file_path}</a>'\n\n    # Display the HTML link\n    display(HTML(html))\n\n    # Create and run the JavaScript to automatically click the link\n    js_code = f'''\n    var link = document.getElementById('download_link_{file_path}_{my_id}');\n    link.click();\n    '''\n    display(Javascript(js_code))\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:47.267553Z","iopub.execute_input":"2025-03-28T09:01:47.268068Z","iopub.status.idle":"2025-03-28T09:01:47.276832Z","shell.execute_reply.started":"2025-03-28T09:01:47.268017Z","shell.execute_reply":"2025-03-28T09:01:47.275487Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## These data only have upto 2022","metadata":{}},{"cell_type":"code","source":"import joblib\nimport pandas as pd\nimport numpy as np\nimport torch\n\n# Load datasets\nhigh_res = pd.read_csv('/kaggle/input/test-gldas-data/resolution_2k.csv').drop(columns=['Unnamed: 0'])\nlow_res = pd.read_csv('/kaggle/input/test-gldas-data/gldas_data_yearwise.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:47.278647Z","iopub.execute_input":"2025-03-28T09:01:47.279164Z","iopub.status.idle":"2025-03-28T09:01:54.572395Z","shell.execute_reply.started":"2025-03-28T09:01:47.279091Z","shell.execute_reply":"2025-03-28T09:01:54.571179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # low_res.columns=low_res.columns.str.replace('_',' ')\n# # Replace underscores with spaces and add \"Representative\" prefix\n# low_res.columns = 'Representative ' + low_res.columns.str.replace('_', ' ')\n\n# low_res.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:54.574485Z","iopub.execute_input":"2025-03-28T09:01:54.574836Z","iopub.status.idle":"2025-03-28T09:01:54.578705Z","shell.execute_reply.started":"2025-03-28T09:01:54.574801Z","shell.execute_reply":"2025-03-28T09:01:54.577612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"joblib.load('/kaggle/input/test-wights-for-groundhog/standard_scaler.pkl').feature_names_in_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:01:54.580436Z","iopub.execute_input":"2025-03-28T09:01:54.580924Z","iopub.status.idle":"2025-03-28T09:01:54.592425Z","shell.execute_reply.started":"2025-03-28T09:01:54.580873Z","shell.execute_reply":"2025-03-28T09:01:54.591336Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downstream task with upsampling model\n## You can choose which points you want to test\n- ### you can test 2 km uniform grid center points\n- ### or you can test the points where in-situ measurements are usually taken","metadata":{}},{"cell_type":"code","source":"import joblib\nimport pandas as pd\nimport numpy as np\nimport torch\n\n# Load datasets\n# high_res = pd.read_csv('/kaggle/input/test-gldas-data/resolution_2k.csv')\n# low_res = pd.read_csv('/kaggle/input/test-gldas-data/gldas_data_yearwise.csv')\nchoice = input(\"Select for which scenario you want to see the results for:\\n (1 for uniform 2km resolution grid points, 2 for in-situ measuring points): \")\n\nif choice == \"1\":\n    high_res = new_high\nelif choice == \"2\":\n    high_res = original\nelse:\n    print(\"Invalid choice. Defaulting to uniform 2km resolution grid points.\")\n    high_res = new_high\n\nlow_res=new_low\n# Load scalers and model\nfitted_scaler = joblib.load('/kaggle/input/test-wights-for-groundhog/fitted_scaler.pkl')\nstandard_scaler = joblib.load('/kaggle/input/test-wights-for-groundhog/standard_scaler.pkl')\nrf_model = joblib.load('/kaggle/input/test-wights-for-groundhog/upscaling_model.joblib')\n\n# standard_scaler = joblib.load('/kaggle/input/weights-for-upsampling-2/scaler_for_high_low.joblib')\n# rf_model = joblib.load('/kaggle/input/weights-for-upsampling-2/upscaling_model.joblib')\n\n# standard_scaler = joblib.load('/kaggle/input/updated-upsampling-weights/scaler_for_high_low.joblib')\n# rf_model = joblib.load('/kaggle/input/updated-upsampling-weights/upscaling_model (1).joblib')\n\nhigh_res['Original_Sy']=high_res.Sy\n# Transform high-resolution data\nhigh_res[fitted_scaler.feature_names_in_] = fitted_scaler.transform(high_res[fitted_scaler.feature_names_in_])\n\n# Merge datasets\ndata = high_res.merge(low_res, on=['GLDAS_SerialID', 'Year']).rename(columns={'POINT_X_x': 'POINT_X', 'POINT_Y_x': 'POINT_Y'})\n\n# Scale numerical features\nX_numerical_scaled = standard_scaler.transform(data[standard_scaler.feature_names_in_])\n# print(data.lithology.unique(),high_res.lithology.unique())\n# Encode categorical data\nX_cat = data[['lithology', 'lithology_MAJORITY']].astype('string')\nX_cat_encoded = pd.get_dummies(X_cat)\n# print(X_cat_encoded.columns)\n\n# print(X_numerical_scaled.shape, X_cat_encoded.shape)\n# Prepare final input tensor\nX_final = torch.tensor(np.hstack([X_numerical_scaled, X_cat_encoded]).astype('float32'))\nprint(X_numerical_scaled.shape, X_cat_encoded.shape)\n# Predict\npred = rf_model.predict(X_final)\nhigh_res['Sy'] = high_res['Sy'] if 'Sy' in high_res.columns else 1  # Ensure Sy column exists\nrecharge = (pred[:, 1] - pred[:, 0]) * 100 * data['Original_Sy']\n\n# Display feature names and feature importance\nfeature_names = list(standard_scaler.feature_names_in_) + list(X_cat_encoded.columns)\nfeature_importance = rf_model.feature_importances_\n\n# Create DataFrame for easier viewing\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# Display feature importance\nprint(importance_df)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T09:01:54.594093Z","iopub.execute_input":"2025-03-28T09:01:54.594893Z","iopub.status.idle":"2025-03-28T09:03:59.782375Z","shell.execute_reply.started":"2025-03-28T09:01:54.594844Z","shell.execute_reply":"2025-03-28T09:03:59.780671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 'lithology_1', 'lithology_10', 'lithology_11', 'lithology_12',\n#        'lithology_13', 'lithology_14', 'lithology_15', 'lithology_16',\n#        'lithology_17', 'lithology_18', 'lithology_19', 'lithology_2',\n#        'lithology_20', 'lithology_21', 'lithology_22', 'lithology_23',\n#        'lithology_24', 'lithology_25', 'lithology_26', 'lithology_27',\n#        'lithology_3', 'lithology_4', 'lithology_5', 'lithology_6',\n#        'lithology_7', 'lithology_8', 'lithology_9', 'lithology_MAJORITY_1',\n#        'lithology_MAJORITY_10', 'lithology_MAJORITY_11',\n#        'lithology_MAJORITY_12', 'lithology_MAJORITY_14',\n#        'lithology_MAJORITY_15', 'lithology_MAJORITY_16',\n#        'lithology_MAJORITY_17', 'lithology_MAJORITY_18',\n#        'lithology_MAJORITY_19', 'lithology_MAJORITY_2',\n#        'lithology_MAJORITY_20', 'lithology_MAJORITY_22',\n#        'lithology_MAJORITY_23', 'lithology_MAJORITY_24',\n#        'lithology_MAJORITY_26', 'lithology_MAJORITY_27',\n#        'lithology_MAJORITY_3', 'lithology_MAJORITY_4', 'lithology_MAJORITY_5',\n#        'lithology_MAJORITY_6', 'lithology_MAJORITY_7', 'lithology_MAJORITY_8',\n#        'lithology_MAJORITY_9'],","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:03:59.784750Z","iopub.execute_input":"2025-03-28T09:03:59.785215Z","iopub.status.idle":"2025-03-28T09:03:59.791463Z","shell.execute_reply.started":"2025-03-28T09:03:59.785173Z","shell.execute_reply":"2025-03-28T09:03:59.790246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names=['TWI', 'TRI', 'Sy', 'STI', 'SPI', 'Slope', 'Profile curvature',\n       'Plan curvature', 'lithology clay thickness',\n       'Distance from stream', 'elevation', 'drainage density',\n       'Curvature', 'Aspect', 'mean NDVI', 'mean NDWI', 'Representative Curvature',\n       'Representative Slope','Representative Profile curvature', 'Representative Plan curvature',\n       'Representative Distance from stream', 'Representative Aspect', 'Representative drainage density',\n       'Representative Elevation', 'Representative SPI', 'Representative STI',\n       'Representative Sy', 'Representative TRI', 'Representative TWI',\n       'Representative lithology clay thickness', 'Representative NDVI', 'Representative NDWI', 'Min GWS (GLDAS)',\n       'Max GWS (GLDAS)']+ list(X_cat_encoded.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:03:59.792768Z","iopub.execute_input":"2025-03-28T09:03:59.793069Z","iopub.status.idle":"2025-03-28T09:03:59.798959Z","shell.execute_reply.started":"2025-03-28T09:03:59.793039Z","shell.execute_reply":"2025-03-28T09:03:59.797794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(['TWI', 'TRI', 'Sy', 'STI', 'SPI', 'Slope', 'Profile curvature',\n       'Plan curvature', 'lithology clay thickness',\n       'Distance from stream', 'elevation', 'drainage density',\n       'Curvature', 'Aspect', 'mean NDVI', 'mean NDWI', 'Representative Curvature',\n       'Representative Slope','Representative Profile curvature', 'Representative Plan curvature',\n       'Representative Distance from stream (MAJORITY)', 'Representative Aspect (MAJORITY) ', 'Representative drainage density MAJORITY',\n       'Representative Elevation (MAJORITY)', 'Representative SPI (MAJORITY)', 'Representative STI (MAJORITY)',\n       'Representative Sy (MAJORITY)', 'Representative TRI (MAJORITY)', 'Representative TWI (MAJORITY)',\n       'Representative lithology clay thickness (MAJORITY)', 'Representative NDVI (MAJORITY)', 'Representative NDWI (MAJORITY)', 'Min GWS (GLDAS)',\n       'Max GWS (GLDAS)']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:03:59.800765Z","iopub.execute_input":"2025-03-28T09:03:59.801257Z","iopub.status.idle":"2025-03-28T09:03:59.809414Z","shell.execute_reply.started":"2025-03-28T09:03:59.801201Z","shell.execute_reply":"2025-03-28T09:03:59.807970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n_,X_test=train_test_split(X_final,test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:03:59.810914Z","iopub.execute_input":"2025-03-28T09:03:59.811322Z","iopub.status.idle":"2025-03-28T09:04:01.195588Z","shell.execute_reply.started":"2025-03-28T09:03:59.811285Z","shell.execute_reply":"2025-03-28T09:04:01.194592Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SHAP for the upsampling model","metadata":{}},{"cell_type":"code","source":"import shap\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Initialize SHAP TreeExplainer with approximate mode\nexplainer = shap.TreeExplainer(rf_model, approximate=True, feature_names=feature_names)\n\n# Compute SHAP values for X_test\nshap_values = explainer.shap_values(X_test.numpy(), approximate=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:04:01.196966Z","iopub.execute_input":"2025-03-28T09:04:01.197359Z","iopub.status.idle":"2025-03-28T09:05:14.566743Z","shell.execute_reply.started":"2025-03-28T09:04:01.197321Z","shell.execute_reply":"2025-03-28T09:05:14.565514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.array(shap_values).shape,X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:05:14.568502Z","iopub.execute_input":"2025-03-28T09:05:14.569062Z","iopub.status.idle":"2025-03-28T09:05:14.671663Z","shell.execute_reply.started":"2025-03-28T09:05:14.569025Z","shell.execute_reply":"2025-03-28T09:05:14.670489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport shap\n# Set font size globally using matplotlib\nplt.rcParams.update({\n    'font.size': 20,      # General font size\n    'axes.titlesize': 20, # Title font size\n    'axes.labelsize': 20, # Axis label font size\n    'xtick.labelsize': 20, # X-tick label size\n    'ytick.labelsize': 16, # Y-tick label size\n})\n# Create a bar plot\nplt.figure(figsize=(10, 10))\n# Generate the summary plot for the first 16 features\nshap.summary_plot(\n    shap_values[1][:, :34],  # Only the first 16 features\n    X_test.numpy()[:, :34],  # Only the first 16 features\n    feature_names=feature_names[:34],  # Names of the first 16 features\n    # matplotlib=True,\n     max_display=34,  # Show all 34 features\n    show=False  # Prevent the plot from being displayed\n)\n# Customize the y-axis tick labels to align them to the left\n# plt.gca().set_yticklabels(plt.gca().get_yticklabels(), ha='left')\n\n# # Adjust the layout to ensure labels fit well\n# plt.gcf().subplots_adjust(left=0.9)  # Increase left margin to avoid truncation\n\n\n# Save the figure\nplt.savefig(\"upsample_max_shap_summary_plot.png\", dpi=300, bbox_inches='tight')  # Save as PNG with high resolution\nplt.close()  # Close the plot to avoid display\n\ntrigger_download('upsample_max_shap_summary_plot.png')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:05:14.673053Z","iopub.execute_input":"2025-03-28T09:05:14.673552Z","iopub.status.idle":"2025-03-28T09:06:31.837656Z","shell.execute_reply.started":"2025-03-28T09:05:14.673501Z","shell.execute_reply":"2025-03-28T09:06:31.836382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Assign those Values","metadata":{}},{"cell_type":"code","source":"data['Max GWL']=pred[:,1]\ndata['Min GWL']=pred[:,0]\ndata['recharge']=recharge","metadata":{"execution":{"iopub.status.busy":"2025-03-28T09:06:31.839349Z","iopub.execute_input":"2025-03-28T09:06:31.839832Z","iopub.status.idle":"2025-03-28T09:06:31.852257Z","shell.execute_reply.started":"2025-03-28T09:06:31.839793Z","shell.execute_reply":"2025-03-28T09:06:31.851103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[['District', 'Upazila']]=original[['District', 'Upazila']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:31.853747Z","iopub.execute_input":"2025-03-28T09:06:31.854158Z","iopub.status.idle":"2025-03-28T09:06:31.914897Z","shell.execute_reply.started":"2025-03-28T09:06:31.854092Z","shell.execute_reply":"2025-03-28T09:06:31.913761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:31.916598Z","iopub.execute_input":"2025-03-28T09:06:31.917048Z","iopub.status.idle":"2025-03-28T09:06:32.425906Z","shell.execute_reply.started":"2025-03-28T09:06:31.917001Z","shell.execute_reply":"2025-03-28T09:06:32.424712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[data.POINT_X<90].sort_values(by=['Max GWL'], ascending=False)[['POINT_X', 'POINT_Y','District','Max GWL','Year' ]].head(10).values.tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:32.427610Z","iopub.execute_input":"2025-03-28T09:06:32.427972Z","iopub.status.idle":"2025-03-28T09:06:32.944088Z","shell.execute_reply.started":"2025-03-28T09:06:32.427938Z","shell.execute_reply":"2025-03-28T09:06:32.942864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Ensure data is sorted properly\ndata = data.sort_values(by=[\"POINT_X\", \"POINT_Y\", \"Year\"])\n\n# Get first and last year's values for each location\nfirst_values = data.groupby([\"POINT_X\", \"POINT_Y\"]).first().reset_index()\nlast_values = data.groupby([\"POINT_X\", \"POINT_Y\"]).last().reset_index()\n\n# Compute absolute change for Max GWL\nfirst_values[\"Max_GWL_Change\"] = last_values[\"Max GWL\"] - first_values[\"Max GWL\"]\n\n# Store the Max GWL values for first and last year\nfirst_values[\"First_Year_Max_GWL\"] = first_values[\"Max GWL\"]\nfirst_values[\"Last_Year_Max_GWL\"] = last_values[\"Max GWL\"]\n\n# Keep relevant columns: Max_GWL_Change, District, First_Year_Max_GWL, and Last_Year_Max_GWL\nresult = first_values[[\"POINT_X\", \"POINT_Y\", \"District\", \"Max_GWL_Change\", \"First_Year_Max_GWL\", \"Last_Year_Max_GWL\"]]\n\n# Get overall statistics\nmax_change = result[\"Max_GWL_Change\"].max()\nmin_change = result[\"Max_GWL_Change\"].min()\navg_change = result[\"Max_GWL_Change\"].mean()\nstd_change = result[\"Max_GWL_Change\"].std()\n\n# Find the locations with max and min change\nmax_change_location = result[result[\"Max_GWL_Change\"] == max_change]\nmin_change_location = result[result[\"Max_GWL_Change\"] == min_change]\n\n# Print results\nprint(f\"Maximum Change in Max GWL: {max_change:.2f} at:\")\nprint(max_change_location)\n\nprint(f\"\\nMinimum Change in Max GWL: {min_change:.2f} at:\")\nprint(min_change_location)\n\nprint(f\"\\nAverage Change in Max GWL: {avg_change:.2f}\")\nprint(f\"Standard Deviation in Max GWL: {std_change:.2f}\")\n\n# Aggregate by district to find where changes are highest\ndistrict_changes = result.groupby(\"District\")[\"Max_GWL_Change\"].mean().sort_values(ascending=False)\n\nprint(\"\\nTop Districts with the Most Change in Max GWL:\")\nprint(district_changes.head(10))  # Show top 10 districts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:32.953714Z","iopub.execute_input":"2025-03-28T09:06:32.954200Z","iopub.status.idle":"2025-03-28T09:06:34.680596Z","shell.execute_reply.started":"2025-03-28T09:06:32.954156Z","shell.execute_reply":"2025-03-28T09:06:34.679331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import theilslopes\n# Sample dataframe\ndf = data  # Replace with actual data\n\n# Group data by POINT_X and POINT_Y and calculate Sen's Slope in one step\ndef calculate_sens_slope(group):\n    if group['Year'].nunique() > 1:\n        slope, intercept, lower, upper = theilslopes(group['recharge'], group['Year'])\n        return round(slope, 2)  # Round slope to 2 decimal places\n    return np.nan  # Not enough data\n\n# Calculate the Sen's Slope for each group and reset index\ntrends_df = df.groupby(['POINT_X', 'POINT_Y']).apply(calculate_sens_slope).reset_index(name='sens_slope')\n\n# Drop rows with NaN values in sens_slope\ntrends_df = trends_df.dropna(subset=['sens_slope'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:06:34.681970Z","iopub.execute_input":"2025-03-28T09:06:34.682335Z","iopub.status.idle":"2025-03-28T09:07:02.479731Z","shell.execute_reply.started":"2025-03-28T09:06:34.682302Z","shell.execute_reply":"2025-03-28T09:07:02.478455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trends_df[(trends_df.POINT_X==90.4167)&(trends_df.POINT_Y==23.7458)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:07:02.481080Z","iopub.execute_input":"2025-03-28T09:07:02.481434Z","iopub.status.idle":"2025-03-28T09:07:02.492278Z","shell.execute_reply.started":"2025-03-28T09:07:02.481400Z","shell.execute_reply":"2025-03-28T09:07:02.491103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[(data.POINT_X==90.4167)&(data.POINT_Y==23.7458)&((data.Year==2003)|(data.Year==2024))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:07:02.493762Z","iopub.execute_input":"2025-03-28T09:07:02.494138Z","iopub.status.idle":"2025-03-28T09:07:02.515313Z","shell.execute_reply.started":"2025-03-28T09:07:02.494082Z","shell.execute_reply":"2025-03-28T09:07:02.514072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[['Year']].head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:07:02.517098Z","iopub.execute_input":"2025-03-28T09:07:02.517724Z","iopub.status.idle":"2025-03-28T09:07:02.530727Z","shell.execute_reply.started":"2025-03-28T09:07:02.517662Z","shell.execute_reply":"2025-03-28T09:07:02.529205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Ensure data is sorted properly\ndata = data.sort_values(by=[\"POINT_X\", \"POINT_Y\", \"Year\"])\n\n# Get first and last year's values for each location\nfirst_values = data.groupby([\"POINT_X\", \"POINT_Y\"]).first().reset_index()\nlast_values = data.groupby([\"POINT_X\", \"POINT_Y\"]).last().reset_index()\n\n# Compute absolute change\nfirst_values[\"recharge_Change\"] = last_values[\"recharge\"] - first_values[\"recharge\"]\n\n# Keep relevant columns: Max_GWL_Change and District\nresult = first_values[[\"POINT_X\", \"POINT_Y\", \"District\", \"recharge_Change\"]]\n\n# Get overall statistics\nmax_change = result[\"recharge_Change\"][result.District==\"Dhaka\"].max()\nmin_change = result[\"recharge_Change\"][result.District==\"Dhaka\"].min()\navg_change = result[\"recharge_Change\"].mean()\nstd_change = result[\"recharge_Change\"].std()\n\n# Find the locations with max and min change\nmax_change_location = result[result[\"recharge_Change\"] == max_change]\nmin_change_location = result[result[\"recharge_Change\"] == min_change]\n\n# Print results\nprint(f\"Maximum Change in recharge: {max_change:.2f} at:\")\nprint(max_change_location)\n\nprint(f\"\\nMinimum Change in recharge : {min_change:.2f} at:\")\nprint(min_change_location)\n\nprint(f\"\\nAverage Change in recharge: {avg_change:.2f}\")\nprint(f\"Standard Deviation in recharge: {std_change:.2f}\")\n\n# Aggregate by district to find where changes are highest\ndistrict_changes = result.groupby(\"District\")[\"recharge_Change\"].mean().sort_values(ascending=False)\n\nprint(\"\\nTop Districts with the Most Change in recharge:\")\nprint(district_changes.head(10))  # Show top 10 districts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:07:02.532404Z","iopub.execute_input":"2025-03-28T09:07:02.532886Z","iopub.status.idle":"2025-03-28T09:07:04.241039Z","shell.execute_reply.started":"2025-03-28T09:07:02.532835Z","shell.execute_reply":"2025-03-28T09:07:04.239371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Ensure data is sorted properly\ndata = data.sort_values(by=[\"POINT_X\", \"POINT_Y\", \"Year\"])\n\n# Get first and last year's values for each location\nfirst_values = data.groupby([\"POINT_X\", \"POINT_Y\"]).first().reset_index()\nlast_values = data.groupby([\"POINT_X\", \"POINT_Y\"]).last().reset_index()\n\n# Compute absolute change for Min GWL\nfirst_values[\"Min_GWL_Change\"] = last_values[\"Min GWL\"] - first_values[\"Min GWL\"]\n\n# Keep relevant columns: Min_GWL_Change and District\nresult = first_values[[\"POINT_X\", \"POINT_Y\", \"District\", \"Min_GWL_Change\"]]\n\n# Get overall statistics\nmax_change = result[\"Min_GWL_Change\"].max()\nmin_change = result[\"Min_GWL_Change\"].min()\navg_change = result[\"Min_GWL_Change\"].mean()\nstd_change = result[\"Min_GWL_Change\"].std()\n\n# Find the locations with max and min change\nmax_change_location = result[result[\"Min_GWL_Change\"] == max_change]\nmin_change_location = result[result[\"Min_GWL_Change\"] == min_change]\n\n# Print results\nprint(f\"Maximum Change in Min GWL: {max_change:.2f} at:\")\nprint(max_change_location)\n\nprint(f\"\\nMinimum Change in Min GWL: {min_change:.2f} at:\")\nprint(min_change_location)\n\nprint(f\"\\nAverage Change in Min GWL: {avg_change:.2f}\")\nprint(f\"Standard Deviation in Min GWL: {std_change:.2f}\")\n\n# Aggregate by district to find where changes are highest\ndistrict_changes = result.groupby(\"District\")[\"Min_GWL_Change\"].mean().sort_values(ascending=False)\n\nprint(\"\\nTop Districts with the Most Change in Min GWL:\")\nprint(district_changes.head(10))  # Show top 10 districts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:07:04.243329Z","iopub.execute_input":"2025-03-28T09:07:04.243738Z","iopub.status.idle":"2025-03-28T09:07:05.496082Z","shell.execute_reply.started":"2025-03-28T09:07:04.243701Z","shell.execute_reply":"2025-03-28T09:07:05.494822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install --upgrade geopy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:07:05.497564Z","iopub.execute_input":"2025-03-28T09:07:05.497919Z","iopub.status.idle":"2025-03-28T09:07:05.502590Z","shell.execute_reply.started":"2025-03-28T09:07:05.497885Z","shell.execute_reply":"2025-03-28T09:07:05.501420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from geopy.geocoders import Nominatim\n# import time\n\n# # List of coordinates in [longitude, latitude] format.\n# coords = [\n#     [90.6064, 23.9494],\n#     [90.5486, 23.9619],\n#     [90.4614, 23.8314],\n#     [90.5989, 23.7844],\n#     [90.4167, 23.7458],\n#     [90.391776, 23.759484]\n# ]\n\n# # Initialize Nominatim with a custom user_agent.\n# geolocator = Nominatim(user_agent=\"geoapiExercises\", timeout=10)\n\n# def reverse_geocode(lon, lat, retries=3):\n#     for attempt in range(retries):\n#         try:\n#             # Note: geopy expects coordinates as (latitude, longitude)\n#             location = geolocator.reverse((lat, lon), exactly_one=True)\n#             if location and location.address:\n#                 return location.address\n#             else:\n#                 return \"Address not found\"\n#         except Exception as e:\n#             print(f\"Attempt {attempt + 1} failed with error: {e}\")\n#             time.sleep(2)  # wait before retrying\n#     return \"Failed to retrieve address after retries\"\n\n# # Loop through coordinates and print the result.\n# for pair in coords:\n#     lon, lat = pair  # Original order: [longitude, latitude]\n#     address = reverse_geocode(lon, lat)\n#     print(f\"Coordinates (lat, lon): ({lat}, {lon})\")\n#     print(\"Address:\", address)\n#     print(\"-\" * 50)\n#     time.sleep(1.5)  # extra delay to respect rate limits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:07:05.503974Z","iopub.execute_input":"2025-03-28T09:07:05.504338Z","iopub.status.idle":"2025-03-28T09:07:05.509534Z","shell.execute_reply.started":"2025-03-28T09:07:05.504305Z","shell.execute_reply":"2025-03-28T09:07:05.508384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.to_csv('predicted_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:07:05.510911Z","iopub.execute_input":"2025-03-28T09:07:05.511290Z","iopub.status.idle":"2025-03-28T09:08:18.482482Z","shell.execute_reply.started":"2025-03-28T09:07:05.511255Z","shell.execute_reply":"2025-03-28T09:08:18.481107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trigger_download('predicted_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:08:18.483960Z","iopub.execute_input":"2025-03-28T09:08:18.484379Z","iopub.status.idle":"2025-03-28T09:08:18.493102Z","shell.execute_reply.started":"2025-03-28T09:08:18.484343Z","shell.execute_reply":"2025-03-28T09:08:18.491906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualiztion","metadata":{}},{"cell_type":"markdown","source":"## Function for plotting\n\n### The bar plot is only applicable if you have choosen the uniform 2km points. As we assumed each point is of 2km x 2km. So, a total of 4 km area in each point","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndef plot_column_as_points(data, column_values, column_name, point_size=5):\n    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n\n    # Extract values for the current column\n    longitudes = data.POINT_X\n    latitudes = data.POINT_Y\n\n    # Define your ranges and corresponding colors\n    boundaries = [0, 5.3, 7.6, 9.8, 11.3, 15, 20.5, 26, 35.5, 58, 68, 78]\n    cmap_colors = [\n        'blue',         # below 0\n        '#a8e1e0',      # 0 to < 5.3 (light blue)\n        '#66c18a',      # 5.3 to < 7.6 (light green)\n        '#3b7a3d',      # 7.6 to < 9.8 (dark green)\n        '#f3d5a4',      # 9.8 to < 11.3 (light purple)\n        '#b299ca',      # 11.3 to < 15 (purple)\n        '#e4a6a3',      # 15 to < 20.5\n        '#d35d60',      # 20.5 to < 26 (red)\n        '#a0322e',      # 26 to < 35.5 (dark red)\n        '#88322e',      # 35.5 to < 58\n        '#55322e',      # 58 to < 68\n        '#330e0f',      # 68 to < 78 (dark gray)\n    ]\n\n    # Create colormap\n    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", cmap_colors)\n    norm = BoundaryNorm(boundaries, cmap.N)\n\n    # Scatter plot\n    sc = ax.scatter(longitudes, latitudes, c=column_values, cmap=cmap, norm=norm, s=30, edgecolor='None', alpha=1.0)\n    cbar = plt.colorbar(sc, ax=ax)\n    cbar.set_label(f'{column_name}', fontsize=22.5)  # Increased fontsize\n    cbar.ax.tick_params(labelsize=19)  # Increase colorbar tick size\n\n    ax.set_xlabel('Longitude', fontsize=22.5)\n    ax.set_ylabel('Latitude', fontsize=22.5)\n    ax.set_title(f'{column_name} ',fontsize=27)\n    ax.tick_params(axis='both', labelsize=20)  # Increase tick labels size\n\n    # Bin the data\n    labels = [f\"{boundaries[i]}-{boundaries[i+1]}\" for i in range(len(boundaries) - 1)]\n    data['Range'] = pd.cut(column_values, bins=boundaries, labels=labels, right=False)\n\n    # Count occurrences and calculate area\n    range_counts = data['Range'].value_counts().reindex(labels, fill_value=0)\n    areas = (range_counts * 4) / 1000  # Convert km² to thousand km²\n    total_area = areas.sum()\n\n    # Create inset bar plot inside scatter plot (top-right corner)\n    ax_inset = inset_axes(ax, width=\"45%\", height=\"23%\", loc='upper right')\n\n    bars = ax_inset.bar(range_counts.index, areas, color=cmap_colors[:len(range_counts) + 1])\n    ax_inset.set_xticklabels(range_counts.index, rotation=45, ha='right', fontsize=12)  # Increased fontsize\n    ax_inset.set_ylabel('Area (1000 km²)', fontsize=15)\n\n    # Add area values and percentages on top of bars\n    for bar, area in zip(bars, areas):\n        height = bar.get_height()\n        if height > 0:\n            percentage = (area / total_area) * 100\n            ax_inset.text(bar.get_x() + bar.get_width() / 2, height + 0.2, \n                          f\"{area:.2f}\\n({percentage:.1f}%)\", ha='center', va='bottom', \n                          fontsize=12, fontweight='bold')  # Increased fontsize\n\n    plt.tight_layout()\n    plt.savefig(f'{column_name}_scatter_inset_bar.png', dpi=300)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:08:18.494592Z","iopub.execute_input":"2025-03-28T09:08:18.494946Z","iopub.status.idle":"2025-03-28T09:08:18.540053Z","shell.execute_reply.started":"2025-03-28T09:08:18.494912Z","shell.execute_reply":"2025-03-28T09:08:18.538552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Maximum and Minimum GWL (BGL) in meters","metadata":{}},{"cell_type":"code","source":"cur_data=data\n# cur_data=psuedo_data\nfor year in cur_data.Year.unique():\n#     if year<=2001:\n#         continue\n    \n    for mode in ['Min GWL','Max GWL']:\n        A=cur_data[cur_data.Year==year].sort_values(by=['POINT_X','POINT_Y'])\n        B=cur_data[cur_data.Year==year-1].sort_values(by=['POINT_X','POINT_Y'])\n        x=A[mode].values.reshape(-1)\n#         y=B['min_gwl'].values.reshape(-1)\n        print(A.shape,x.shape)\n        plot_column_as_points(A,x,f'Upsample {mode} (BGL) for the year {year} (meters)')","metadata":{"execution":{"iopub.status.busy":"2025-03-28T09:08:18.541752Z","iopub.execute_input":"2025-03-28T09:08:18.542162Z","iopub.status.idle":"2025-03-28T09:12:12.746056Z","shell.execute_reply.started":"2025-03-28T09:08:18.542111Z","shell.execute_reply":"2025-03-28T09:12:12.744659Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Function for plotting GLDAS\n\n### The bar plot is only applicable if you have choosen the uniform 2km points. As we assumed each point is of 2km x 2km. So, a total of 4 km area in each point","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n\ndef plot_column_as_points(data, column_values, column_name,point_size=5):\n    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n\n    # Extract values for the current column\n    longitudes = data.POINT_X\n    latitudes = data.POINT_Y\n\n    # Define your ranges and corresponding colors\n    boundaries = [0, 500, 700, 900, 1100, 1300, 1500, 1700, 1900,2000 ]\n    cmap_colors = [\n        'blue',         # below 0\n        '#a8e1e0',      # < 5.3 (light blue)\n        '#66c18a',      # 5.3 to < 7.6 (light green)\n        '#3b7a3d',      # 7.6 to < 9.8 (dark green)\n        '#f3d5a4',      # 9.8 to < 11.3 (light purple)\n        '#b299ca',      # 11.3 to < 15 (purple)\n        '#e4a6a3',      # 15 to < 20.5 (-0.5 to 0d)\n        '#d35d60',      # 20.5 to < 26 (red)\n        '#a0322e',      # 26 to < 35.5 (dark red)\n        '#330e0f',      # 35.5 to < 58 (dark gray)\n    ]\n\n    # Create a custom colormap using LinearSegmentedColormap\n    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", cmap_colors)\n    norm = BoundaryNorm(boundaries, cmap.N)\n\n    # # Scatter plot with discrete color mapping\n    # sc = ax.scatter(longitudes, latitudes, c=column_values, cmap=cmap, norm=norm, s=point_size, edgecolor='None')\n\n    # # Add a color bar\n    # cbar = plt.colorbar(sc, ax=ax)\n    # cbar.set_label(f'{column_name}')\n\n    # Scatter plot\n    sc = ax.scatter(longitudes, latitudes, c=column_values, cmap=cmap, norm=norm, s=30, edgecolor='None', alpha=1.0)\n    cbar = plt.colorbar(sc, ax=ax)\n    cbar.set_label(f'{column_name}', fontsize=22.5)  # Increased fontsize\n    cbar.ax.tick_params(labelsize=19)  # Increase colorbar tick size\n\n    ax.set_xlabel('Longitude', fontsize=22.5)\n    ax.set_ylabel('Latitude', fontsize=22.5)\n    ax.set_title(f'{column_name} ',fontsize=27)\n    ax.tick_params(axis='both', labelsize=20)  # Increase tick labels size\n    # Bin the data\n    labels = [f\"{boundaries[i]}-{boundaries[i+1]}\" for i in range(len(boundaries) - 1)]\n    data['Range'] = pd.cut(column_values, bins=boundaries, labels=labels, right=False)\n\n    # Count occurrences and calculate area\n    range_counts = data['Range'].value_counts().reindex(labels, fill_value=0)\n    areas = (range_counts * 4) / 1000  # Convert km² to thousand km²\n    total_area = areas.sum()\n\n    # Create inset bar plot inside scatter plot (top-right corner)\n    ax_inset = inset_axes(ax, width=\"45%\", height=\"23%\", loc='upper right')\n\n    bars = ax_inset.bar(range_counts.index, areas, color=cmap_colors[:len(range_counts) + 1])\n    ax_inset.set_xticklabels(range_counts.index, rotation=45, ha='right', fontsize=12)  # Increased fontsize\n    ax_inset.set_ylabel('Area (1000 km²)', fontsize=15)\n\n    # Add area values and percentages on top of bars\n    for bar, area in zip(bars, areas):\n        height = bar.get_height()\n        if height > 0:\n            percentage = (area / total_area) * 100\n            ax_inset.text(bar.get_x() + bar.get_width() / 2, height + 0.2, \n                          f\"{area:.2f}\\n({percentage:.1f}%)\", ha='center', va='bottom', \n                          fontsize=12, fontweight='bold')  # Increased fontsize\n\n    # Save the plot to file\n    plt.savefig(f'{column_name}.png', dpi=300)\n    print(column_name)\n    plt.show()\n\n# Example Usage\n# Assuming data is a DataFrame containing column_name values and 'longitudes', 'latitudes' as separate arrays\n# plot_column_as_points(data, column_values, 'GWL_Column')\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T09:12:12.748362Z","iopub.execute_input":"2025-03-28T09:12:12.749101Z","iopub.status.idle":"2025-03-28T09:12:12.770969Z","shell.execute_reply.started":"2025-03-28T09:12:12.749046Z","shell.execute_reply":"2025-03-28T09:12:12.769545Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"low_res.Max_GWS.max()","metadata":{"execution":{"iopub.status.busy":"2025-03-28T09:12:12.772567Z","iopub.execute_input":"2025-03-28T09:12:12.772995Z","iopub.status.idle":"2025-03-28T09:12:12.780971Z","shell.execute_reply.started":"2025-03-28T09:12:12.772957Z","shell.execute_reply":"2025-03-28T09:12:12.779704Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cur_data=data\n# cur_data=psuedo_data\nfor year in cur_data.Year.unique():\n#     if year<=2001:\n#         continue\n    \n    for mode in ['Min_GWS','Max_GWS']:\n        A=cur_data[cur_data.Year==year].sort_values(by=['POINT_X','POINT_Y'])\n        B=cur_data[cur_data.Year==year-1].sort_values(by=['POINT_X','POINT_Y'])\n        x=A[mode].values.reshape(-1)\n#         y=B['min_gwl'].values.reshape(-1)\n        print(A.shape,x.shape)\n        plot_column_as_points(A,x,f'GLDAS low res {mode.replace(\"_\",\" \")}  for the year {year} (mm)')","metadata":{"execution":{"iopub.status.busy":"2025-03-28T09:12:12.782460Z","iopub.execute_input":"2025-03-28T09:12:12.782820Z","iopub.status.idle":"2025-03-28T09:15:32.366087Z","shell.execute_reply.started":"2025-03-28T09:12:12.782787Z","shell.execute_reply":"2025-03-28T09:15:32.364288Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Function for plotting Recharge\n\n### The bar plot is only applicable if you have choosen the uniform 2km points. As we assumed each point is of 2km x 2km. So, a total of 4 km area in each point","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n\ndef plot_column_as_points(data,column_values,column_name):\n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n    # Extract values for the current column\n    longitudes=data.POINT_X\n    latitudes=data.POINT_Y\n\n    # Define your ranges and corresponding colors\n    boundaries = [0, 5.3, 7.6, 9.8, 11.3, 15, 20.5, 26, 35.5, 58, 60, 70, 80, 90, 100, 150]\n    cmap_colors = [\n        'blue',         # below 0\n        '#a8e1e0',      # < 5.3 (light blue)\n        '#66c18a',      # 5.3 to < 7.6 (light green)\n        '#3b7a3d',      # 7.6 to < 9.8 (dark green)\n        '#f3d5a4',      # 9.8 to < 11.3 (light purple)\n        '#b299ca',      # 11.3 to < 15 (purple)\n        '#e4a6a3',      # 15 to < 20.5 (-0.5 to 0d)\n        '#d35d60',      # 20.5 to < 26 (red)\n        '#a0322e',      # 26 to < 35.5 (dark red)\n        '#330e0f',      # 35.5 to < 58 (dark gray)\n        '#4f4d4d',      # 58 to < 60 (gray)\n        '#7d7b7b',      # 60 to < 70 (light gray)\n        '#a9a8a8',      # 70 to < 80 (lighter gray)\n        '#c2c0c0',      # 80 to < 90 (lightest gray)\n        '#dbdbdb',      # 90 to < 100 (almost white)\n        'black'         # 100+ (black)\n    ]\n\n    # Create a custom colormap using LinearSegmentedColormap\n    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", cmap_colors)\n    norm = BoundaryNorm(boundaries, cmap.N)\n\n    # Scatter plot with discrete color mapping\n    sc = ax.scatter(longitudes, latitudes, c=column_values, cmap=cmap, norm=norm, s=5, edgecolor='None')\n\n    # Add a color bar\n    cbar = plt.colorbar(sc, ax=ax)\n    cbar.set_label(f'{column_name} ')\n\n    # Add title and labels\n    ax.set_title(f'{column_name} GWL Points ', fontsize=16)\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n\n    # Save the plot to file\n    plt.savefig(f'{column_name}.png', dpi=300)\n    print(column_name)\n    plt.show()\n\n# Example Usage\n# Assuming data is a DataFrame containing column_name values and 'longitudes', 'latitudes' as separate arrays\n# plot_column_as_points('GWL_Column', data, longitudes, latitudes)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T09:15:32.368179Z","iopub.execute_input":"2025-03-28T09:15:32.368639Z","iopub.status.idle":"2025-03-28T09:15:32.379577Z","shell.execute_reply.started":"2025-03-28T09:15:32.368599Z","shell.execute_reply":"2025-03-28T09:15:32.378184Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\ndef plot_column_with_bar(data, column_values, column_name):\n    fig, ax = plt.subplots(figsize=(20, 20))  # Main scatter plot\n\n    # Extract values for the current column\n    longitudes = data.POINT_X\n    latitudes = data.POINT_Y\n\n    # Define ranges and colors\n    boundaries = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 70, 80, 100]\n    cmap_colors = [\n        'blue', '#a8e1e0', '#66c18a', '#f4a700',  # Changed deep green to deep yellow\n        '#f3d5a4', '#b299ca', '#e4a6a3', '#d35d60',\n        '#a0322e', '#330e0f', '#4f4d4d', '#7d7b7b',\n        '#a9a8a8', '#c2c0c0', '#dbdbdb', 'black'\n    ]\n\n    # Create colormap\n    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", cmap_colors)\n    norm = BoundaryNorm(boundaries, cmap.N)\n\n    # Scatter plot\n    sc = ax.scatter(longitudes, latitudes, c=column_values, cmap=cmap, norm=norm, s=30, edgecolor='None', alpha=1.0)\n    cbar = plt.colorbar(sc, ax=ax)\n    cbar.set_label(f'{column_name}', fontsize=22.5)  # Increased fontsize\n    cbar.ax.tick_params(labelsize=19)  # Increase colorbar tick size\n\n    ax.set_xlabel('Longitude', fontsize=22.5)\n    ax.set_ylabel('Latitude', fontsize=22.5)\n    ax.tick_params(axis='both', labelsize=20)  # Increase tick labels size\n    # Bin the data\n    labels = [f\"{boundaries[i]}-{boundaries[i+1]}\" for i in range(len(boundaries) - 1)]\n    data['Range'] = pd.cut(column_values, bins=boundaries, labels=labels, right=False)\n\n    # Count occurrences and calculate area\n    range_counts = data['Range'].value_counts().reindex(labels, fill_value=0)\n    areas = (range_counts * 4) / 1000  # Convert km² to thousand km²\n    total_area = areas.sum()\n\n    # Create inset bar plot inside scatter plot (top-right corner)\n    ax_inset = inset_axes(ax, width=\"45%\", height=\"23%\", loc='upper right')\n\n    bars = ax_inset.bar(range_counts.index, areas, color=cmap_colors[:len(range_counts) + 1])\n    ax_inset.set_xticklabels(range_counts.index, rotation=45, ha='right', fontsize=12)  # Increased fontsize\n    ax_inset.set_ylabel('Area (1000 km²)', fontsize=15)\n\n    # Add area values and percentages on top of bars\n    for bar, area in zip(bars, areas):\n        height = bar.get_height()\n        if height > 0:\n            percentage = (area / total_area) * 100\n            ax_inset.text(bar.get_x() + bar.get_width() / 2, height + 0.2, \n                          f\"{area:.2f}\\n({percentage:.1f}%)\", ha='center', va='bottom', \n                          fontsize=9, fontweight='bold')  # Increased fontsize\n\n    plt.tight_layout()\n    plt.savefig(f'{column_name}_scatter_inset_bar.png', dpi=300)\n    plt.show()\n\n# Example Usage:\n# plot_column_with_bar(data, data['GWL_Column'], 'GWL_Column')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:15:32.381575Z","iopub.execute_input":"2025-03-28T09:15:32.381963Z","iopub.status.idle":"2025-03-28T09:15:32.397032Z","shell.execute_reply.started":"2025-03-28T09:15:32.381928Z","shell.execute_reply":"2025-03-28T09:15:32.395776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visulaizing The Recharge","metadata":{}},{"cell_type":"code","source":"len(cur_data[cur_data.Year==year])*4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:15:32.398561Z","iopub.execute_input":"2025-03-28T09:15:32.398937Z","iopub.status.idle":"2025-03-28T09:15:32.445041Z","shell.execute_reply.started":"2025-03-28T09:15:32.398896Z","shell.execute_reply":"2025-03-28T09:15:32.443930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cur_data=data\n# cur_data=psuedo_data\nfor year in cur_data.Year.unique():\n\n    A=cur_data[cur_data.Year==year].sort_values(by=['POINT_X','POINT_Y'])\n    B=cur_data[cur_data.Year==year-1].sort_values(by=['POINT_X','POINT_Y'])\n    x=A['recharge'].values.reshape(-1)\n#         y=B['min_gwl'].values.reshape(-1)\n    print(A.shape,x.shape)\n    plot_column_with_bar(A,x,f'Upsample Recharge for the year {year} (centimeters)')\n    # break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:15:32.446380Z","iopub.execute_input":"2025-03-28T09:15:32.446728Z","iopub.status.idle":"2025-03-28T09:17:36.318854Z","shell.execute_reply.started":"2025-03-28T09:15:32.446673Z","shell.execute_reply":"2025-03-28T09:17:36.317544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Zip ","metadata":{}},{"cell_type":"code","source":"!zip GLDAS_GWS.zip *GWS*\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:17:36.320482Z","iopub.execute_input":"2025-03-28T09:17:36.320857Z","iopub.status.idle":"2025-03-28T09:17:39.859295Z","shell.execute_reply.started":"2025-03-28T09:17:36.320822Z","shell.execute_reply":"2025-03-28T09:17:39.857716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trigger_download('GLDAS_GWS.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:17:39.861696Z","iopub.execute_input":"2025-03-28T09:17:39.862236Z","iopub.status.idle":"2025-03-28T09:17:39.872891Z","shell.execute_reply.started":"2025-03-28T09:17:39.862177Z","shell.execute_reply":"2025-03-28T09:17:39.871773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip Upsampled_GWL.zip *GWL*\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:17:39.874780Z","iopub.execute_input":"2025-03-28T09:17:39.875396Z","iopub.status.idle":"2025-03-28T09:17:48.164062Z","shell.execute_reply.started":"2025-03-28T09:17:39.875345Z","shell.execute_reply":"2025-03-28T09:17:48.162505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip Upsampled_Recharge.zip *Recharge*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:17:48.165948Z","iopub.execute_input":"2025-03-28T09:17:48.166378Z","iopub.status.idle":"2025-03-28T09:17:54.445989Z","shell.execute_reply.started":"2025-03-28T09:17:48.166336Z","shell.execute_reply":"2025-03-28T09:17:54.444746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trigger_download('Upsampled_GWL.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:17:54.448166Z","iopub.execute_input":"2025-03-28T09:17:54.448729Z","iopub.status.idle":"2025-03-28T09:17:54.459804Z","shell.execute_reply.started":"2025-03-28T09:17:54.448670Z","shell.execute_reply":"2025-03-28T09:17:54.458490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trigger_download('Upsampled_Recharge.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:17:54.461319Z","iopub.execute_input":"2025-03-28T09:17:54.461673Z","iopub.status.idle":"2025-03-28T09:17:54.470378Z","shell.execute_reply.started":"2025-03-28T09:17:54.461640Z","shell.execute_reply":"2025-03-28T09:17:54.469216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizing the Trend \n\n### The bar plot is only applicable if you have choosen the uniform 2km points. As we assumed each point is of 2km x 2km. So, a total of 4 km area in each point","metadata":{}},{"cell_type":"markdown","source":"## Visualizing the Trend of Recharge","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import theilslopes\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n# Sample dataframe\ndf = data  # Replace with actual data\n\n# Group data by POINT_X and POINT_Y and calculate Sen's Slope in one step\ndef calculate_sens_slope(group):\n    if group['Year'].nunique() > 1:\n        slope, intercept, lower, upper = theilslopes(group['recharge'], group['Year'])\n        return round(slope, 2)  # Round slope to 2 decimal places\n    return np.nan  # Not enough data\n\n# Calculate the Sen's Slope for each group and reset index\ntrends_df = df.groupby(['POINT_X', 'POINT_Y']).apply(calculate_sens_slope).reset_index(name='sens_slope')\n\n# Drop rows with NaN values in sens_slope\ntrends_df = trends_df.dropna(subset=['sens_slope'])\n\n# Define bins and labels for categorizing slope values\nbins = [-1, -0.5, -0.3, -0.05, 0.05, 0.5, 1]\nlabels = ['-1 to -0.5', '-0.5 to -0.3', '-0.3 to -0.05', '-0.05 to 0.05', '0.05 to 0.5', '0.5 to 1']\ntrends_df['slope_category'] = pd.cut(trends_df['sens_slope'], bins=bins, labels=labels, right=False)\n\n# Define the custom color palette for discrete categories\ncustom_palette = {\n    '-1 to -0.5': '#000000',\n    '-0.5 to -0.3': '#BB0000',\n    '-0.3 to -0.05': '#FFA07A',\n    '-0.05 to 0.05': \"#FFFFFF\",\n    '0.05 to 0.5': '#ADD8E6',\n    '0.5 to 1': '#00008B'\n}\n\n# Create figure and scatter plot\nfig, ax = plt.subplots(figsize=(10, 12))\n\nscatter = sns.scatterplot(\n    data=trends_df,\n    x='POINT_X',\n    y='POINT_Y',\n    hue='slope_category',\n    palette=custom_palette,\n    edgecolor=None,\n    ax=ax\n)\n\n# Add title and labels\nax.set_title(\"Recharge Trend (Sen's Slope cm/year) for Bangladesh (2003 to 2022)\", fontsize=16)\nax.set_xlabel('POINT_X')\nax.set_ylabel('POINT_Y')\nax.legend(title=\"Slope Category\", loc='upper right')\n\n# Count occurrences for the bar plot\ncategory_counts = trends_df['slope_category'].value_counts().reindex(labels, fill_value=0)\n\n# Convert counts to area (each point = 4 km²)\ntotal_points = category_counts.sum()\ncategory_area = (category_counts * 4) / 1000  # Convert to thousand km²\ncategory_percent = (category_counts / total_points) * 100  # Calculate percentage\n\n# Create inset bar plot\nax_inset = inset_axes(ax, width=\"50%\", height=\"20%\", loc='upper right')\n\nbars = ax_inset.bar(\n    category_counts.index, \n    category_area,  # Use area (in thousand km²) instead of count\n    color=[custom_palette[label] for label in category_counts.index], \n    edgecolor='black',  # Black edges\n    linewidth=0.5\n)\n\nax_inset.set_xticklabels(category_counts.index, \n                         # rotation=45,\n                         ha='center', fontsize=8)\nax_inset.set_ylabel('Area (1000 km²)', fontsize=10)\n\n\n# Add values on top of bars (percentage + area)\nfor bar, (area, percent) in zip(bars, zip(category_area, category_percent)):\n    height = bar.get_height()\n    if height > 0:\n        ax_inset.text(\n            bar.get_x() + bar.get_width() / 2, height + 1, \n            f\"{area:.1f}k km²\\n{percent:.1f}%\",  # Show area in thousand km² and percentage\n            ha='center', va='bottom', fontsize=8, fontweight='bold'\n        )\nplt.tight_layout()\nplt.savefig('discrete_sens_slope_recharge_trend_with_bar_edges.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:17:54.472469Z","iopub.execute_input":"2025-03-28T09:17:54.472899Z","iopub.status.idle":"2025-03-28T09:18:27.773227Z","shell.execute_reply.started":"2025-03-28T09:17:54.472864Z","shell.execute_reply":"2025-03-28T09:18:27.771988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kendalltau\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n# Sample dataframe\ndf = data  # Replace with actual data\n\n# Group data by POINT_X and POINT_Y and calculate the Mann-Kendall trend\ndef calculate_mann_kendall_trend(group):\n    if group['Year'].nunique() > 1:\n        tau, p_value = kendalltau(group['Year'], group['recharge'])\n        if p_value < 0.05:  # Assuming significance level of 0.05\n            return round(tau, 2)  # Round tau to 2 decimal places\n        else:\n            return 0.0  # Mark as zero trend instead of NaN\n    return 0.0  # Not enough data also marked as zero trend\n\n# Calculate the trend for each group\ntrends_df = df.groupby(['POINT_X', 'POINT_Y']).apply(calculate_mann_kendall_trend).reset_index(name='trend_tau')\n\n# Categorize trends (including zero trend)\ndef categorize_trend(tau):\n    if tau > 0:\n        return 'Positive Trend'\n    elif tau < 0:\n        return 'Negative Trend'\n    else:\n        return 'Zero Trend'  # Mark zero trend explicitly\n\ntrends_df['trend_category'] = trends_df['trend_tau'].apply(categorize_trend)\n\n# Define color palette\ncustom_palette = {\n    'Positive Trend': 'blue',\n    'Negative Trend': 'red',\n    'Zero Trend': 'grey'\n}\n\n# Create main scatter plot\nfig, ax = plt.subplots(figsize=(10, 12))\n\nscatter = sns.scatterplot(\n    data=trends_df,\n    x='POINT_X',\n    y='POINT_Y',\n    hue='trend_category',\n    palette=custom_palette,\n    edgecolor=None,\n    ax=ax\n)\n\n# Add title and labels\nax.set_title('Trend of Upsampled Recharge Change (Mann-Kendall) for Bangladesh (2003-2022)', fontsize=16)\nax.set_xlabel('POINT_X')\nax.set_ylabel('POINT_Y')\nax.legend(title='Trend Category', loc='upper right')\n\n# Count occurrences for bar plot (ensure all categories exist)\ncategory_counts = trends_df['trend_category'].value_counts()\n\n# Ensure all categories exist in the count dictionary\nfor category in ['Positive Trend', 'Negative Trend', 'Zero Trend']:\n    if category not in category_counts:\n        category_counts[category] = 0  # Add missing categories with zero count\n\n# Convert counts to area (each point = 4 km²)\ntotal_points = category_counts.sum()\ncategory_area = (category_counts * 4) / 1000  # Convert to thousand km²\ncategory_percent = (category_counts / total_points) * 100  # Calculate percentage\n\n# Create inset bar plot\nax_inset = inset_axes(ax, width=\"50%\", height=\"20%\", loc='upper right')\n\nbars = ax_inset.bar(\n    category_counts.index, \n    category_area,  # Use area (in thousand km²) instead of count\n    color=[custom_palette[label] for label in category_counts.index], \n    # edgecolor='black',  # Black edges\n    # linewidth=1.2\n)\n\nax_inset.set_xticklabels(category_counts.index, \n                         # rotation=45,\n                         ha='center', fontsize=8)\nax_inset.set_ylabel('Area (1000 km²)', fontsize=10)\n\n# Add values on top of bars (percentage + area)\nfor bar, (area, percent) in zip(bars, zip(category_area, category_percent)):\n    height = bar.get_height()\n    if height > 0:\n        ax_inset.text(\n            bar.get_x() + bar.get_width() / 2, height + 1, \n            f\"{area:.1f}k km²\\n{percent:.1f}%\",  # Show area in thousand km² and percentage\n            ha='center', va='bottom', fontsize=8, fontweight='bold'\n        )\n\nplt.tight_layout()\nplt.savefig('recharge_trend_mann_kendall_with_percentage_area.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:18:27.774928Z","iopub.execute_input":"2025-03-28T09:18:27.775391Z","iopub.status.idle":"2025-03-28T09:18:58.074597Z","shell.execute_reply.started":"2025-03-28T09:18:27.775343Z","shell.execute_reply":"2025-03-28T09:18:58.073307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing the Trend  of Min GWL","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import theilslopes\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n# Sample dataframe\ndf = data  # Replace with actual data\n\n# Group data by POINT_X and POINT_Y and calculate Sen's Slope\ndef calculate_sens_slope(group):\n    if group['Year'].nunique() > 1:\n        slope, intercept, lower, upper = theilslopes(group['Min GWL'], group['Year'])\n        return round(slope, 2)  # Round slope to 2 decimal places\n    return np.nan  # Not enough data\n\n# Calculate the Sen's Slope for each group and reset index\ntrends_df = df.groupby(['POINT_X', 'POINT_Y']).apply(calculate_sens_slope).reset_index(name='sens_slope')\n\n# Drop rows with NaN values\ntrends_df = trends_df.dropna(subset=['sens_slope'])\n\n# Define bins and labels for categorizing slope values\nbins = [-1, -0.5, 0, 0.5, 1]\nlabels = ['-1 to -0.5', '-0.5 to 0', '0 to 0.5', '0.5 to 1']\ntrends_df['slope_category'] = pd.cut(trends_df['sens_slope'], bins=bins, labels=labels)\n\n# Define custom color palette\ncustom_palette = {\n    '-1 to -0.5': '#8B0000',  # Strong negative trend (dark red)\n    '-0.5 to 0': '#FFA07A',   # Moderate negative trend (light salmon)\n    '0 to 0.5': '#ADD8E6',    # Moderate positive trend (light blue)\n    '0.5 to 1': '#00008B'     # Strong positive trend (deep blue)\n}\n\n# Create main scatter plot\nfig, ax = plt.subplots(figsize=(10, 12))\n\nscatter = sns.scatterplot(\n    data=trends_df,\n    x='POINT_X',\n    y='POINT_Y',\n    hue='slope_category',\n    palette=custom_palette,\n    edgecolor=None,\n    ax=ax\n)\n\n# Add title and labels\nax.set_title(\"Min GWL Trend (Sen's Slope meters/year) for Bangladesh (2003 to 2022)\", fontsize=16)\nax.set_xlabel('POINT_X')\nax.set_ylabel('POINT_Y')\nax.legend(title=\"Slope Category\", loc='upper right')\n\n# Count occurrences for bar plot\ncategory_counts = trends_df['slope_category'].value_counts()\n\n# Create inset bar plot\n# ax_inset = inset_axes(ax, width=\"50%\", height=\"23%\", loc='upper right')\n\n# Convert counts to area (each point = 4 km²)\ntotal_points = category_counts.sum()\ncategory_area = (category_counts * 4) / 1000  # Convert to thousand km²\ncategory_percent = (category_counts / total_points) * 100  # Calculate percentage\n\n# Create inset bar plot\nax_inset = inset_axes(ax, width=\"50%\", height=\"20%\", loc='upper right')\n\nbars = ax_inset.bar(\n    category_counts.index, \n    category_area,  # Use area (in thousand km²) instead of count\n    color=[custom_palette[label] for label in category_counts.index], \n    edgecolor='black',  # Black edges\n    linewidth=0.5\n)\n\nax_inset.set_xticklabels(category_counts.index, \n                         # rotation=45,\n                         ha='center', fontsize=8)\nax_inset.set_ylabel('Area (1000 km²)', fontsize=10)\n # Add values on top of bars (percentage + area)\nfor bar, (area, percent) in zip(bars, zip(category_area, category_percent)):\n    height = bar.get_height()\n    if height > 0:\n        ax_inset.text(\n            bar.get_x() + bar.get_width() / 2, height + 1, \n            f\"{area:.1f}k km²\\n{percent:.1f}%\",  # Show area in thousand km² and percentage\n            ha='center', va='bottom', fontsize=8, fontweight='bold'\n        )\nplt.tight_layout()\nplt.savefig('sens_slope_min_gwl_trend_with_bar_edges.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:18:58.076542Z","iopub.execute_input":"2025-03-28T09:18:58.076983Z","iopub.status.idle":"2025-03-28T09:19:30.534605Z","shell.execute_reply.started":"2025-03-28T09:18:58.076942Z","shell.execute_reply":"2025-03-28T09:19:30.533195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kendalltau\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n# Sample dataframe\ndf = data  # Replace with actual data\n\n# Function to calculate the Mann-Kendall trend for each (POINT_X, POINT_Y)\ndef calculate_mann_kendall_trend(group):\n    if group['Year'].nunique() > 1:\n        tau, p_value = kendalltau(group['Year'], group['Min GWL'])\n        return round(tau, 2) if p_value < 0.05 else 0.0  # Use 0.0 for non-significant trends\n    return 0.0  # Not enough data\n\n# Apply trend calculation\ntrends_df = df.groupby(['POINT_X', 'POINT_Y']).apply(calculate_mann_kendall_trend).reset_index(name='trend_tau')\n\n# Categorize trends\ndef categorize_trend(tau):\n    if tau > 0:\n        return 'Positive Trend'\n    elif tau < 0:\n        return 'Negative Trend'\n    else:\n        return 'Zero Trend'\n\ntrends_df['trend_category'] = trends_df['trend_tau'].apply(categorize_trend)\n\n# Define custom colors\ncustom_palette = {'Positive Trend': 'blue', 'Negative Trend': 'red', 'Zero Trend': 'grey'}\n\n# Create main scatter plot\nfig, ax = plt.subplots(figsize=(10, 12))\n\nsns.scatterplot(\n    data=trends_df,\n    x='POINT_X',\n    y='POINT_Y',\n    hue='trend_category',\n    palette=custom_palette,\n    edgecolor=None,\n    ax=ax\n)\n\n# Add labels\nax.set_title('Trend of Upsampled Min GWL Change (Mann-Kendall) for Bangladesh (2003-2022)', fontsize=16)\nax.set_xlabel('POINT_X')\nax.set_ylabel('POINT_Y')\nax.legend(title='Trend Category', loc='upper right')\n\n# Count occurrences of each trend category\ncategory_counts = trends_df['trend_category'].value_counts()\n\n# Ensure all categories exist\nfor category in ['Positive Trend', 'Negative Trend', 'Zero Trend']:\n    if category not in category_counts:\n        category_counts[category] = 0  # Assign zero if missing\n\n# Convert counts to area (each point = 4 km²)\ntotal_points = category_counts.sum()\ncategory_area = (category_counts * 4) / 1000  # Convert to thousand km²\ncategory_percent = (category_counts / total_points) * 100  # Calculate percentage\n\n# Create inset bar plot\nax_inset = inset_axes(ax, width=\"50%\", height=\"20%\", loc='upper right')\n\nbars = ax_inset.bar(\n    category_counts.index, \n    category_area, \n    color=[custom_palette[label] for label in category_counts.index], \n    # edgecolor='black', \n    # linewidth=1.2\n)\n\nax_inset.set_xticklabels(category_counts.index,\n                         # rotation=45,\n                         ha='center', fontsize=8)\nax_inset.set_ylabel('Area (1000 km²)', fontsize=10)\n\n# Add values on top of bars (percentage + area)\nfor bar, (area, percent) in zip(bars, zip(category_area, category_percent)):\n    height = bar.get_height()\n    if height > 0:\n        ax_inset.text(\n            bar.get_x() + bar.get_width() / 2, height + 1, \n            f\"{area:.1f}k km²\\n{percent:.1f}%\",  # Show area in thousand km² and percentage\n            ha='center', va='bottom', fontsize=8, fontweight='bold'\n        )\n\nplt.tight_layout()\nplt.savefig('min_gwl_trend_mann_kendall_with_area.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:19:30.536513Z","iopub.execute_input":"2025-03-28T09:19:30.537069Z","iopub.status.idle":"2025-03-28T09:20:00.007799Z","shell.execute_reply.started":"2025-03-28T09:19:30.537018Z","shell.execute_reply":"2025-03-28T09:20:00.006221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing the Trend  of Max GWL","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import theilslopes\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n# Sample dataframe\ndf = data  # Replace with actual data\n\n# Group data by POINT_X and POINT_Y and calculate Sen's Slope\ndef calculate_sens_slope(group):\n    if group['Year'].nunique() > 1:\n        slope, intercept, lower, upper = theilslopes(group['Max GWL'], group['Year'])\n        return round(slope, 2)  # Round slope to 2 decimal places\n    return np.nan  # Not enough data\n\n# Calculate the Sen's Slope for each group\ntrends_df = df.groupby(['POINT_X', 'POINT_Y']).apply(calculate_sens_slope).reset_index(name='sens_slope')\n\n# Drop rows with NaN values in sens_slope\ntrends_df = trends_df.dropna(subset=['sens_slope'])\n\n# Define bins and labels for categorizing slope values\nbins = [-1, -0.5, 0, 0.5, 1]\nlabels = ['-1 to -0.5', '-0.5 to 0', '0 to 0.5', '0.5 to 1']\ntrends_df['slope_category'] = pd.cut(trends_df['sens_slope'], bins=bins, labels=labels)\n\n# Define custom color palette\ncustom_palette = {\n    '-1 to -0.5': '#8B0000',  # Strong negative trend (dark red)\n    '-0.5 to 0': '#FFA07A',   # Moderate negative trend (light salmon)\n    '0 to 0.5': '#ADD8E6',    # Moderate positive trend (light blue)\n    '0.5 to 1': '#00008B'     # Strong positive trend (deep blue)\n}\n# Create main scatter plot\nfig, ax = plt.subplots(figsize=(10, 12))\n\nscatter = sns.scatterplot(\n    data=trends_df,\n    x='POINT_X',\n    y='POINT_Y',\n    hue='slope_category',\n    palette=custom_palette,\n    edgecolor=None,\n    ax=ax\n)\n\n# Add title and labels\nax.set_title(\"Max GWL Trend (Sen's Slope meters/year) for Bangladesh (2003 to 2022)\", fontsize=16)\nax.set_xlabel('POINT_X')\nax.set_ylabel('POINT_Y')\nax.legend(title=\"Slope Category\", loc='upper right')\n\n# Count occurrences for bar plot (ensure all categories exist)\ncategory_counts = trends_df['slope_category'].value_counts()\n\n# Ensure all categories exist in the count dictionary\nfor category in labels:\n    if category not in category_counts:\n        category_counts[category] = 0  # Add missing categories with zero count\n\n# Convert counts to area in 1000 km² (each point = 4 km²)\ncategory_area = (category_counts * 4) / 1000  # Convert to thousand km²\n\n# Create inset bar plot\nax_inset = inset_axes(ax, width=\"50%\", height=\"23%\", loc='upper right')\n\nbars = ax_inset.bar(\n    category_counts.index, \n    category_area, \n    color=[custom_palette[label] for label in category_counts.index], \n    edgecolor='black',  # Black edges\n    linewidth=0.1\n)\n# Convert counts to area (each point = 4 km²)\ntotal_points = category_counts.sum()\ncategory_area = (category_counts * 4) / 1000  # Convert to thousand km²\ncategory_percent = (category_counts / total_points) * 100  # Calculate percentage\n\n\nax_inset.set_xticklabels(category_counts.index, ha='center', fontsize=8)\nax_inset.set_ylabel('Area (1000 km²)', fontsize=10)\n\n# Add values on top of bars (percentage + area)\nfor bar, (area, percent) in zip(bars, zip(category_area, category_percent)):\n    height = bar.get_height()\n    if height > 0:\n        ax_inset.text(\n            bar.get_x() + bar.get_width() / 2, height + 1, \n            f\"{area:.1f}k km²\\n{percent:.1f}%\",  # Show area in thousand km² and percentage\n            ha='center', va='bottom', fontsize=8, fontweight='bold'\n        )\nplt.tight_layout()\nplt.savefig('discrete_sens_slope_max_gwl_trend_with_area.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:20:00.009615Z","iopub.execute_input":"2025-03-28T09:20:00.010001Z","iopub.status.idle":"2025-03-28T09:20:33.177194Z","shell.execute_reply.started":"2025-03-28T09:20:00.009963Z","shell.execute_reply":"2025-03-28T09:20:33.175843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kendalltau\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n# Sample dataframe\ndf = data  # Replace with actual data\n\n# Group data by POINT_X and POINT_Y and calculate the Mann-Kendall trend\ndef calculate_mann_kendall_trend(group):\n    if group['Year'].nunique() > 1:\n        tau, p_value = kendalltau(group['Year'], group['Max GWL'])\n        return round(tau, 2) if p_value < 0.05 else 0.0  # Use 0.0 for non-significant trends\n    return 0.0  # Not enough data also marked as zero trend\n\n# Calculate the trend for each group and reset index\ntrends_df = df.groupby(['POINT_X', 'POINT_Y']).apply(calculate_mann_kendall_trend).reset_index(name='trend_tau')\n\n# Create a categorical column based on the Mann-Kendall tau value\ndef categorize_trend(tau):\n    if tau > 0:\n        return 'Positive Trend'\n    elif tau < 0:\n        return 'Negative Trend'\n    else:\n        return 'Zero Trend'\n\ntrends_df['trend_category'] = trends_df['trend_tau'].apply(categorize_trend)\n\n# Define the custom color palette for discrete categories\ncustom_palette = {\n    'Positive Trend': 'blue',\n    'Negative Trend': 'red',\n    'Zero Trend': 'grey'\n}\n\n# Create main scatter plot\nfig, ax = plt.subplots(figsize=(10, 12))\n\nscatter = sns.scatterplot(\n    data=trends_df,\n    x='POINT_X',\n    y='POINT_Y',\n    hue='trend_category',\n    palette=custom_palette,\n    edgecolor=None,\n    ax=ax\n)\n\n# Add title and labels\nax.set_title('Trend of Upsampled Max GWL Change (Mann-Kendall) for Bangladesh (2003-2022)', fontsize=16)\nax.set_xlabel('POINT_X')\nax.set_ylabel('POINT_Y')\nax.legend(title='Trend Category', loc='upper right')\n\n# Count occurrences for bar plot (ensure all categories exist)\ncategory_counts = trends_df['trend_category'].value_counts()\n\n# Ensure all categories exist in the count dictionary\nfor category in ['Positive Trend', 'Negative Trend', 'Zero Trend']:\n    if category not in category_counts:\n        category_counts[category] = 0  # Add missing categories with zero count\n\n# Convert counts to area (each point = 4 km²)\ncategory_area = (category_counts * 4) / 1000  # Convert to thousand km²\n\n# Create inset bar plot\nax_inset = inset_axes(ax, width=\"50%\", height=\"23%\", loc='upper right')\n\nbars = ax_inset.bar(\n    category_counts.index, \n    category_area, \n    color=[custom_palette[label] for label in category_counts.index], \n    # edgecolor='black',  # Black edges\n    # linewidth=1.2\n)\n\nax_inset.set_xticklabels(category_counts.index, ha='center', fontsize=8)\nax_inset.set_ylabel('Area (1000 km²)', fontsize=10)\n\n# Add values on top of bars (percentage + area)\nfor bar, (area, percent) in zip(bars, zip(category_area, category_percent)):\n    height = bar.get_height()\n    if height > 0:\n        ax_inset.text(\n            bar.get_x() + bar.get_width() / 2, height + 1, \n            f\"{area:.1f}k km²\\n{percent:.1f}%\",  # Show area in thousand km² and percentage\n            ha='center', va='bottom', fontsize=8, fontweight='bold'\n        )\nplt.tight_layout()\nplt.savefig('max_gwl_trend_mann_kendall_with_area.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:20:33.179075Z","iopub.execute_input":"2025-03-28T09:20:33.179510Z","iopub.status.idle":"2025-03-28T09:21:03.550793Z","shell.execute_reply.started":"2025-03-28T09:20:33.179470Z","shell.execute_reply":"2025-03-28T09:21:03.549484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip SensSlope.zip *sen*\n!zip MannKandall.zip *mann_kandall*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:03.552451Z","iopub.execute_input":"2025-03-28T09:21:03.553196Z","iopub.status.idle":"2025-03-28T09:21:06.664869Z","shell.execute_reply.started":"2025-03-28T09:21:03.553140Z","shell.execute_reply":"2025-03-28T09:21:06.663233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trigger_download('SensSlope.zip')\ntrigger_download('MannKandall.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:06.666941Z","iopub.execute_input":"2025-03-28T09:21:06.667371Z","iopub.status.idle":"2025-03-28T09:21:06.681545Z","shell.execute_reply.started":"2025-03-28T09:21:06.667332Z","shell.execute_reply":"2025-03-28T09:21:06.680240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trigger_download('discrete_sens_slope_recharge_trend.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:06.683149Z","iopub.execute_input":"2025-03-28T09:21:06.683658Z","iopub.status.idle":"2025-03-28T09:21:06.688729Z","shell.execute_reply.started":"2025-03-28T09:21:06.683615Z","shell.execute_reply":"2025-03-28T09:21:06.687554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[['POINT_X','POINT_Y','Max GWL','Min GWL','recharge','Year','GLDAS_SerialID']].to_csv('Upsampled_2km_resolution_GWL.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:06.690219Z","iopub.execute_input":"2025-03-28T09:21:06.690580Z","iopub.status.idle":"2025-03-28T09:21:13.016517Z","shell.execute_reply.started":"2025-03-28T09:21:06.690547Z","shell.execute_reply":"2025-03-28T09:21:13.015231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"upsampled_gwl=data[['POINT_X','POINT_Y','Max GWL','Min GWL','recharge','Year','GLDAS_SerialID']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:13.018067Z","iopub.execute_input":"2025-03-28T09:21:13.018447Z","iopub.status.idle":"2025-03-28T09:21:13.032785Z","shell.execute_reply.started":"2025-03-28T09:21:13.018411Z","shell.execute_reply":"2025-03-28T09:21:13.031564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"upsampled_gwl[(upsampled_gwl['Max GWL']>60)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:13.034210Z","iopub.execute_input":"2025-03-28T09:21:13.034631Z","iopub.status.idle":"2025-03-28T09:21:13.047632Z","shell.execute_reply.started":"2025-03-28T09:21:13.034596Z","shell.execute_reply":"2025-03-28T09:21:13.046377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q rasterio shapely","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:13.049240Z","iopub.execute_input":"2025-03-28T09:21:13.049758Z","iopub.status.idle":"2025-03-28T09:21:26.364759Z","shell.execute_reply.started":"2025-03-28T09:21:13.049707Z","shell.execute_reply":"2025-03-28T09:21:26.362938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import rasterio\n\n# Open the raster file\nwith rasterio.open(\"/kaggle/input/water-table-ratio/LOG_WTR_L_01.tif\") as src:\n    # Print basic metadata\n    print(\"File Metadata:\")\n    print(\"----------------\")\n    print(f\"File Name: {src.name}\")\n    print(f\"CRS: {src.crs}\")\n    print(f\"Width, Height: {src.width} x {src.height}\")\n    print(f\"Number of Bands: {src.count}\")\n    print(f\"Data Type: {src.dtypes[0]}\")\n    print(f\"Bounds: {src.bounds}\")\n    print(f\"Resolution: {src.res}\")\n    \n    # Read additional metadata\n    print(\"\\nAdditional Metadata:\")\n    for key, value in src.meta.items():\n        print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:26.367217Z","iopub.execute_input":"2025-03-28T09:21:26.367793Z","iopub.status.idle":"2025-03-28T09:21:27.273166Z","shell.execute_reply.started":"2025-03-28T09:21:26.367734Z","shell.execute_reply":"2025-03-28T09:21:27.271491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import rasterio\nimport pandas as pd\nfrom rasterio.sample import sample_gen\nfrom shapely.geometry import Point\n\n# Load your DataFrame\n# Assuming data has columns 'POINT_X' and 'POINT_Y' representing coordinates\n# data = pd.read_csv('your_data.csv')  # Uncomment to load data from a file\n\n# Open the recharge raster file\nwith rasterio.open(\"/kaggle/input/water-table-ratio/LOG_WTR_L_01.tif\") as src:\n    # Extract the coordinates from the DataFrame\n    coords = [(x, y) for x, y in zip(data['POINT_X'], data['POINT_Y'])]\n    \n    # Sample the raster at these coordinates\n    recharge_values = [val[0] for val in src.sample(coords)]\n    \n# Add the recharge values as a new column in the DataFrame\ndata['Recharge_from_'] = recharge_values\n\n# Display the DataFrame with the new 'Recharge' column\nprint(data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:21:27.275003Z","iopub.execute_input":"2025-03-28T09:21:27.275977Z","iopub.status.idle":"2025-03-28T09:22:29.978095Z","shell.execute_reply.started":"2025-03-28T09:21:27.275920Z","shell.execute_reply":"2025-03-28T09:22:29.976860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trigger_download('Upsampled_2km_resolution_GWL.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:22:29.979833Z","iopub.execute_input":"2025-03-28T09:22:29.980375Z","iopub.status.idle":"2025-03-28T09:22:29.990152Z","shell.execute_reply.started":"2025-03-28T09:22:29.980320Z","shell.execute_reply":"2025-03-28T09:22:29.988908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" pd.get_dummies(data['lithology']).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:22:29.992052Z","iopub.execute_input":"2025-03-28T09:22:29.992586Z","iopub.status.idle":"2025-03-28T09:22:30.022073Z","shell.execute_reply.started":"2025-03-28T09:22:29.992535Z","shell.execute_reply":"2025-03-28T09:22:30.020608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"importance_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:22:30.023759Z","iopub.execute_input":"2025-03-28T09:22:30.024187Z","iopub.status.idle":"2025-03-28T09:22:30.032679Z","shell.execute_reply.started":"2025-03-28T09:22:30.024138Z","shell.execute_reply":"2025-03-28T09:22:30.030497Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summing the importance of all the lithology for better interpretation","metadata":{}},{"cell_type":"code","source":"# Create DataFrame for feature importance\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n\n# Filter only for encoded categorical features\nencoded_importance_df = importance_df[importance_df['Feature'].isin(X_cat_encoded.columns)].copy()\n\n# Initialize a dictionary to hold summed importances\nsummed_importance = {}\n\n# Sum importance for features starting with 'lithology'\nsummed_importance['lithology'] = encoded_importance_df[~encoded_importance_df['Feature'].str.startswith('lithology_MAJORITY')]['Importance'].sum()\n\n# Sum importance for features starting with 'lithology_MAJORITY'\nsummed_importance['Representative lithology'] = encoded_importance_df[encoded_importance_df['Feature'].str.startswith('lithology_MAJORITY')]['Importance'].sum()\n\n# Convert the results to a DataFrame for easier viewing\nsummed_importance_df = pd.DataFrame(summed_importance.items(), columns=['Feature', 'Total Importance'])\n\n# Display the summed feature importances\nprint(summed_importance_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:22:30.034536Z","iopub.execute_input":"2025-03-28T09:22:30.034934Z","iopub.status.idle":"2025-03-28T09:22:30.049660Z","shell.execute_reply.started":"2025-03-28T09:22:30.034897Z","shell.execute_reply":"2025-03-28T09:22:30.048314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"importance_df_2=pd.concat([importance_df[~importance_df['Feature'].isin(X_cat_encoded.columns)],summed_importance_df.rename(columns={'Total Importance':'Importance'})],axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:22:30.051398Z","iopub.execute_input":"2025-03-28T09:22:30.051774Z","iopub.status.idle":"2025-03-28T09:22:30.059815Z","shell.execute_reply.started":"2025-03-28T09:22:30.051740Z","shell.execute_reply":"2025-03-28T09:22:30.058564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"importance_df_2.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:22:30.061539Z","iopub.execute_input":"2025-03-28T09:22:30.061944Z","iopub.status.idle":"2025-03-28T09:22:30.069915Z","shell.execute_reply.started":"2025-03-28T09:22:30.061909Z","shell.execute_reply":"2025-03-28T09:22:30.068578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# summed_importance_df.drop(columns=['lithology_MAJORITY'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:22:30.071331Z","iopub.execute_input":"2025-03-28T09:22:30.071758Z","iopub.status.idle":"2025-03-28T09:22:30.076664Z","shell.execute_reply.started":"2025-03-28T09:22:30.071724Z","shell.execute_reply":"2025-03-28T09:22:30.075238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport numpy as np\n\n# Assuming importance_df_2 is defined and has the columns 'Feature' and 'Importance'\n# Sort the DataFrame by 'Importance' in ascending order\nimportance_df_2_sorted = importance_df_2.sort_values(by='Importance', ascending=True)\n\n# Calculate the percentage of each importance value\ntotal_importance = importance_df_2_sorted['Importance'].sum()\nimportance_df_2_sorted['Percentage'] = (importance_df_2_sorted['Importance'] / total_importance) * 100\n\n# Generate a gradient of colors from blue to red\ngradient_colors = [mcolors.to_hex(c) for c in plt.cm.coolwarm(np.linspace(0, 1, len(importance_df_2_sorted)))]\n\n# Create a bar plot\nplt.figure(figsize=(10, 17))\n\n# Adjust bar positions to increase spacing between ticks\ny_positions = np.arange(len(importance_df_2_sorted))  # Original positions\nspacing = 1  # Adjust this value for more spacing\ny_positions_spaced = y_positions * spacing\n\nbars = plt.barh(\n    y_positions_spaced, \n    importance_df_2_sorted['Importance'], \n    color=gradient_colors\n)\n\n# Annotate bars with percentages\nfor i, bar in enumerate(bars):\n    plt.text(\n        bar.get_width() + 0.02,  # Position to the right of the bar\n        bar.get_y() + bar.get_height() / 2,  # Center of the bar\n        f\"{importance_df_2_sorted['Percentage'].iloc[i]:.1f}%\",  # Format percentage to 1 decimal place\n        va='center',  # Vertically centered\n        fontsize=12,  # Font size for the annotation\n        color='black'  # Text color\n    )\n\n# Replace y-ticks with spaced positions and corresponding labels\nplt.yticks(y_positions_spaced, importance_df_2_sorted['Feature'], fontsize=12)\n\n# Customize axis labels and title\nplt.xlabel('Importance', fontsize=14)\nplt.ylabel('Features', fontsize=14)\nplt.title('Upsampling Model Feature Importance', fontsize=16)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\n# Extend x-axis for better visibility\nplt.xlim(0, importance_df_2_sorted['Importance'].max() * 1.1)\n\n# Add grid lines\nplt.grid(axis='x', linestyle='--', alpha=0.7)\n\n# Adjust layout for better fit\nplt.tight_layout()\n\n# Save the figure with DPI 300\nplt.savefig('Upsampling_feature_importance.png', dpi=300)\n\n# Display the plot\nplt.show()\ntrigger_download('Upsampling_feature_importance.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T09:22:30.078331Z","iopub.execute_input":"2025-03-28T09:22:30.078736Z","iopub.status.idle":"2025-03-28T09:22:32.427443Z","shell.execute_reply.started":"2025-03-28T09:22:30.078702Z","shell.execute_reply":"2025-03-28T09:22:32.426139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}