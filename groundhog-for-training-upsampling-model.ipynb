{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":9907344,"datasetId":6086909,"databundleVersionId":10164716},{"sourceType":"datasetVersion","sourceId":11151667,"datasetId":5774548,"databundleVersionId":11549678},{"sourceType":"datasetVersion","sourceId":11083478,"datasetId":5905465,"databundleVersionId":11472591},{"sourceType":"datasetVersion","sourceId":9776457,"datasetId":5931106,"databundleVersionId":10018707}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the openpyxl library (if not already installed)\n!pip install -q openpyxl\n!pip install -q xlrd\n!pip install -q sdv\n# Import pandas\nimport pandas as pd\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-03-29T16:50:18.038087Z","iopub.execute_input":"2025-03-29T16:50:18.038414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# This notebook trains the upsampling Model","metadata":{}},{"cell_type":"markdown","source":"# Trigger Download","metadata":{}},{"cell_type":"code","source":"from IPython.display import FileLink, display, HTML, Javascript\n\ndef trigger_download(filename, my_id=0):\n    # Create the FileLink object\n    file_link = FileLink(filename)\n\n    # Get the path of the file\n    file_path = file_link.path\n\n    # Create the HTML link\n    html = f'<a id=\"download_link_{file_path}_{my_id}\" href=\"{file_path}\" download>{file_path}</a>'\n\n    # Display the HTML link\n    display(HTML(html))\n\n    # Create and run the JavaScript to automatically click the link\n    js_code = f'''\n    var link = document.getElementById('download_link_{file_path}_{my_id}');\n    link.click();\n    '''\n    display(Javascript(js_code))\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save CheckPoint","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef save_checkpoint(model, optimizer, lr_scheduler, filename='checkpoint.pth'):\n    \"\"\"\n    Saves the model, optimizer, and learning rate scheduler states to a file.\n\n    Args:\n        model (torch.nn.Module): The model to save.\n        optimizer (torch.optim.Optimizer): The optimizer whose state should be saved.\n        lr_scheduler (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler.\n        filename (str): The path to the file where the checkpoint will be saved.\n    \"\"\"\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': lr_scheduler.state_dict(),\n    }, filename)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load CheckPoint","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef load_checkpoint(model, optimizer, lr_scheduler, filename='checkpoint.pth'):\n    \"\"\"\n    Loads the model, optimizer, and learning rate scheduler states from a file.\n\n    Args:\n        model (torch.nn.Module): The model to load the state into.\n        optimizer (torch.optim.Optimizer): The optimizer to load the state into.\n        lr_scheduler (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler to load the state into.\n        filename (str): The path to the file where the checkpoint is saved.\n    \"\"\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read original data","metadata":{}},{"cell_type":"code","source":"# original_ground_truth=pd.read_csv(\"/kaggle/input/gldas-fixed-data/original_ground_truth.csv\",low_memory=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# plt.hist(np.array(original_ground_truth[['Min-gwl_2005']].values,dtype='float32'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# original_ground_truth.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pseudo_ground_truth=pd.read_csv('/kaggle/input/gldas-fixed-data/pseudo_ground_truth.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# plt.hist(np.array(pseudo_ground_truth[['Max-gwl_2005']].values,dtype='float32'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Keeping only threshold pseudo data","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from geopy.distance import geodesic\n# from scipy.spatial import KDTree\n\n# # Function to calculate geodesic distance between two points (in km)\n# def haversine_distance(lat1, lon1, lat2, lon2):\n#     R = 6371.0  # Radius of the Earth in km\n#     phi1, phi2 = np.radians(lat1), np.radians(lat2)\n#     delta_phi = np.radians(lat2 - lat1)\n#     delta_lambda = np.radians(lon2 - lon1)\n    \n#     a = np.sin(delta_phi / 2) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2) ** 2\n#     c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n#     return R * c  # in kilometers\n\n# def filter_by_distance_kdtree(original_df, psuedo_df, distance_km=2):\n#     # Convert the latitude and longitude to radians for KDTree\n#     original_coords = np.radians(original_df[['POINT_Y', 'POINT_X']].values)  # (Lat, Lng)\n#     psuedo_coords = np.radians(psuedo_df[['POINT_Y', 'POINT_X']].values)      # (Lat, Lng)\n    \n#     # Create a KDTree for the original coordinates\n#     tree = KDTree(original_coords)\n    \n#     # Query the KDTree for all psuedo points to find points within the radius\n#     # The radius here is converted to radians as well (distance_km / Earth's radius in km)\n#     radius = distance_km / 6371.0  # Earth's radius in km\n#     far_enough = np.full(len(psuedo_coords), True)  # Initialize all as True\n    \n#     for i, psuedo_point in enumerate(psuedo_coords):\n#         # Query the tree to find if there are any points within the given radius\n#         idx = tree.query_ball_point(psuedo_point, radius)\n#         if idx:  # If there are any nearby points, mark this point as False\n#             far_enough[i] = False\n    \n#     # Filter the psuedo dataframe for points that are far enough from all original points\n#     filtered_psuedo_df = psuedo_df[far_enough]\n    \n#     # Return the original dataframe and the filtered pseudo dataframe\n#     return original_df, filtered_psuedo_df\n\n# # Usage example\n# original_filtered_df, psuedo_filtered_df = filter_by_distance_kdtree(original_ground_truth, pseudo_ground_truth,1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filtered=pd.concat([original_filtered_df, psuedo_filtered_df])\n# filtered.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# original_filtered_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered=pd.read_csv('/kaggle/input/original-and-pseudo-gwl-data/original_and_pseudo_gwl (2).csv').drop(columns=['recharge'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting data yearwise ","metadata":{}},{"cell_type":"markdown","source":"## Funxtion for splitting the data into year wise","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef reshape_by_year(df):\n    \"\"\"\n    Reshape the input dataframe so that each row corresponds to a specific year with \n    NDVI, NDWI, Min-GWL, and Max-GWL values for that year.\n    \n    Parameters:\n    df (pd.DataFrame): Original dataframe containing the yearly data in separate columns.\n    \n    Returns:\n    pd.DataFrame: Reshaped dataframe with one row per year containing NDVI, NDWI, Min-GWL, and Max-GWL.\n    \"\"\"\n    # Step 1: Create a list of years\n    years = list(range(2003, 2023))  # Assuming the years range from 2001 to 2023\n\n    # Step 2: Prepare a dataframe for each year with NDWI, NDVI, and Min/Max GWL\n    reshaped_df = pd.DataFrame()\n#     print(df.shape)\n    for year in years:\n        # Filter the columns related to the current year\n        year_columns = [f'NDVI{year}', f'NDWI{year}', f'Min-gwl_{year}', f'Max-gwl_{year}']\n        \n        # Create a temporary dataframe for the current year\n        year_df = df[[ 'POINT_X', 'POINT_Y', \n                       'TWI', 'TRI', 'Sy', 'STI', 'SPI', 'Slope', \n                      'Profile_curvature', 'Plan_curvature', 'lithology', 'lithology_clay_thickness', \n                      'Distance_from_stream', 'elevation', 'drainage_density', 'Curvature', \n                      'Aspect', 'GLDAS_SerialID']].copy()\n\n        # Add the NDVI, NDWI, and GWL columns for the current year\n        year_df['NDVI'] = df[f'NDVI{year}']\n        year_df['NDWI'] = df[f'NDWI{year}']\n        year_df['Min_GWL'] = df[f'Min-gwl_{year}']\n        year_df['Max_GWL'] = df[f'Max-gwl_{year}']\n        \n        # Add the year as a column\n        year_df['Year'] = year\n#         print(reshaped_df.shape)\n        # Append the temporary dataframe to the final dataframe\n        reshaped_df = pd.concat([reshaped_df, year_df], ignore_index=True)\n\n    # Step 3: Return the reshaped dataframe\n    return reshaped_df\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# melted","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reading GLDAS","metadata":{}},{"cell_type":"code","source":"gldas_ndvi=pd.read_excel('/kaggle/input/gldas-fixed-data/GLDAS1_Gridwise_ndvi_ndwi_24Sept2024.xls')\ngldas_hgf=pd.read_excel('/kaggle/input/gldas-fixed-data/GLDAS1_Gridwise_14HgfValue_flood_clay_and GWL 18Sept2024.xls').rename(columns={'X':'POINT_X','Y':'POINT_Y'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gldas_ndvi.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gldas_pre=gldas_hgf.merge(gldas_ndvi,on=['SerialID'],how='inner').drop(columns=['drainage density_corrected','ORIG_FID','ID']).rename(columns={'SerialID':'GLDAS_SerialID'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in gldas_pre.columns[~gldas_pre.columns.str.contains('GWS')&~gldas_pre.columns.str.contains('nd')]:\n    print(i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef reshape_by_year(df):\n    \"\"\"\n    Reshape the input dataframe so that each row corresponds to a specific year with\n    NDVI, NDWI, Min-GWL, and Max-GWL values for that year.\n\n    Parameters:\n    df (pd.DataFrame): Original dataframe containing the yearly data in separate columns.\n\n    Returns:\n    pd.DataFrame: Reshaped dataframe with one row per year containing NDVI, NDWI, Min-GWL, and Max-GWL.\n    \"\"\"\n    # Step 1: Create a list of years\n    years = list(range(2003, 2023))  # Assuming the years range from 2001 to 2023\n\n    # Step 2: Prepare a dataframe for each year with NDWI, NDVI, and Min/Max GWL\n    reshaped_df = pd.DataFrame()\n#     print(df.shape)\n    for year in years:\n        # Filter the columns related to the current year\n        year_columns = [f'ndvi{year}', f'ndwi{year}', f'Min-gwl_{year}', f'Max-gwl_{year}']\n        \n        # Create a temporary dataframe for the current year\n        year_df = df[gldas_pre.columns[~gldas_pre.columns.str.contains('GWS')&~gldas_pre.columns.str.contains('nd')]\n                    ].copy()\n\n        # Add the NDVI, NDWI, and GWL columns for the current year\n        year_df['NDVI'] = df[f'ndvi{year}']\n        year_df['NDWI'] = df[f'ndwi{year}']\n        year_df['Min_GWS'] = df[f'Min_GWS_{year}']\n        year_df['Max_GWS'] = df[f'Max_GWS_{year}']\n\n        # Add the year as a column\n        year_df['Year'] = year\n#         print(reshaped_df.shape)\n        # Append the temporary dataframe to the final dataframe\n        reshaped_df = pd.concat([reshaped_df, year_df], ignore_index=True)\n\n    # Step 3: Return the reshaped dataframe\n    return reshaped_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gldas=reshape_by_year(gldas_pre)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gldas","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import BoundaryNorm, LinearSegmentedColormap\n\ndef plot_column_as_points(data,column_values,column_name):\n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\n    # Extract values for the current column\n    longitudes=data.POINT_X\n    latitudes=data.POINT_Y\n\n    # Define your ranges and corresponding colors\n    boundaries = [0, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1550, 1600, 1700, 1800]\n    cmap_colors = [\n        'blue',         # below 0\n        '#a8e1e0',      # < 5.3 (light blue)\n        '#66c18a',      # 5.3 to < 7.6 (light green)\n        '#3b7a3d',      # 7.6 to < 9.8 (dark green)\n        '#f3d5a4',      # 9.8 to < 11.3 (light purple)\n        '#b299ca',      # 11.3 to < 15 (purple)\n        '#e4a6a3',      # 15 to < 20.5 (light red)\n        '#d35d60',      # 20.5 to < 26 (red)\n        '#a0322e',      # 26 to < 35.5 (dark red)\n        '#330e0f',      # 35.5 to < 58 (dark gray)\n        '#4f4d4d',      # 58 to < 60 (gray)\n        '#7d7b7b',      # 60 to < 70 (light gray)\n        '#a9a8a8',      # 70 to < 80 (lighter gray)\n        '#c2c0c0',      # 80 to < 90 (lightest gray)\n        '#dbdbdb',      # 90 to < 100 (almost white)\n        'black'         # 100+ (black)\n    ]\n\n    # Create a custom colormap using LinearSegmentedColormap\n    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", cmap_colors)\n    norm = BoundaryNorm(boundaries, cmap.N)\n\n    # Scatter plot with discrete color mapping\n    sc = ax.scatter(longitudes, latitudes, c=column_values, cmap=cmap, norm=norm, s=100, edgecolor='None')\n\n    # Add a color bar\n    cbar = plt.colorbar(sc, ax=ax)\n    cbar.set_label(f'{column_name} (m)')\n\n    # Add title and labels\n    ax.set_title(f'{column_name} GWL Points ')\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n\n    # Save the plot to file\n    plt.savefig(f'{column_name}.png', dpi=300)\n    print(column_name)\n    plt.show()\n\n# Example Usage\n# Assuming data is a DataFrame containing column_name values and 'longitudes', 'latitudes' as separate arrays\n# plot_column_as_points('GWL_Column', data, longitudes, latitudes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -r *","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # cur_data=original_with_no_nans\n# cur_data=gldas\n\n# for mode in gldas.columns[gldas.columns.str.contains('GWS')].tolist():\n#     A=cur_data\n# #     B=cur_data[cur_data.Year==year-1].sort_values(by=['POINT_X','POINT_Y'])\n#     x=A[mode].values.reshape(-1)\n# #         y=B['min_gwl'].values.reshape(-1)\n#     print(A.shape,x.shape)\n#     year=mode[8:]\n# #     print(year)\n#     if year[0]==\"_\":\n#         year=\"0\"+year[1]\n# #     print(year)\n#     plot_column_as_points(A,x,f'GLDAS {mode[:7]} for the year {year}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !zip GLDAS.zip *GLDAS*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trigger_download('GLDAS.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'gldas' is your dataframe\n# Check for 'ND' in each column and create a list of columns containing 'ND'\n\ncolumns_with_nd = gldas.columns[gldas.columns.str.contains('ND')]\n\nprint(\"Columns containing 'ND':\", columns_with_nd)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gldas[columns_with_nd].isnull().any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 1: Create a SimpleImputer with the desired strategy\n# imputer = KNNImputer(n_neighbors=25, weights=\"distance\")\n# temp=imputer.fit_transform(gldas[columns_with_nd])\n# gldas.drop(columns=columns_with_nd)\n# gldas[columns_with_nd]=temp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# gldas_split = reshape_by_year(gldas)\n# gldas_split.head()\n# gldas_split.rename(columns={'Min_GWL':'Min_GWS','Max_GWL':'Max_GWS'}\n#                       ,inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gldas_split=gldas","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming reshaped_data is your DataFrame\nfiltered['GLDAS_SerialID'] = filtered['GLDAS_SerialID'].astype(int)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered[['GLDAS_SerialID']].isnull().any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered['lithology'] = filtered['lithology'].astype('string')\ngldas_split['lithology_MAJORITY'] = gldas_split['lithology_MAJORITY'].astype('string')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gldas_split.to_csv('gldas_data_yearwise.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trigger_download('gldas_data_yearwise.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data=filtered.merge(gldas_split,on=['GLDAS_SerialID','Year'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in data.columns:\n    print(i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data['Distance_x']=data['POINT_X_y']-data['POINT_X_x']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data['Distance_y']=data['POINT_Y_y']-data['POINT_Y_x']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2=data.drop(columns=['POINT_X_x','POINT_Y_x','POINT_X_y','POINT_Y_y','GLDAS_SerialID','Year'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data2.TWI_y=data2.TWI_y.astype('float')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2.isnull().any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data2.isnull().any()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sdv.metadata import Metadata\n\n# metadata = Metadata.detect_from_dataframe(\n#     data=data2,\n# #     table_name='hotel_guests'\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making X and Y","metadata":{}},{"cell_type":"code","source":"X=data2.drop(columns=['Min_GWL','Max_GWL'])\ny=data2[['Min_GWL','Max_GWL']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_numerical=X.drop(columns=['lithology','lithology_MAJORITY','Unnamed: 0'])\nX_cat=X[['lithology','lithology_MAJORITY']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_numerical.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Assume X_continuous is your continuous data with missing values\n\n\n# Step 3: Normalize the imputed data\nscaler = StandardScaler()\nX_numerical_scaled = scaler.fit_transform(X_numerical)\n\n# X_continuous_scaled is now imputed and normalized\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_numerical.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nX_cat_encoded = pd.get_dummies(X_cat)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nX_final=torch.tensor(np.hstack([X_numerical_scaled,X_cat_encoded]).astype('float32'))\ny_final=torch.tensor(y.values.astype('float32'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_all=np.hstack([scaler.transform(data[X_numerical.columns]),pd.get_dummies(data[X_cat.columns])])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_columns=X_numerical.columns.tolist()+pd.get_dummies(data[X_cat.columns]).columns.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_all=torch.tensor(X_all.astype('float32'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_all=torch.tensor(data[y.columns].values.astype('float32'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_final,y_final,test_size=0.2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Upsampling Model","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Create a random forest regressor\nupscaling_model = RandomForestRegressor(n_estimators=100, random_state=42,verbose=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training and evaluating","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nupscaling_model.fit(X_train,y_train)\ny_pred=upscaling_model.predict(X_test)\n# Assuming y_test and y_pred are your true and predicted values respectively\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"R² Score: {r2}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom joblib import dump\n\n# Save the model without compression\ndump(upscaling_model, 'upscaling_model.joblib')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dump(scaler, 'scaler_for_high_low.joblib')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trigger_download('upscaling_model.joblib')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trigger_download('scaler_for_high_low.joblib')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('ok')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Leave one year out Test","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport numpy as np\nfrom tqdm.auto import tqdm\n# Assuming X_final and y_final are your features and target respectively\n# And data is your DataFrame that contains a 'Year' column\n\n# Initialize the random forest regressor\nupscaling_model = RandomForestRegressor(n_estimators=100, random_state=42, verbose=1)\n\n# Initialize lists to store results\nr2_scores = [0.8856239845780784, 0.9227938708715682, 0.9083272343276982, 0.939034429440263, 0.9548519513365312, 0.9661974545820067, 0.9656912385710076, 0.966248767800651, 0.951333204728009, 0.9700662067387675, 0.9554049692552401]\n\nmse_scores = [0.7767543078251055, 0.5070831298818431, 0.6576105896314403, 0.46002426578169486, 0.3617002412091861, 0.3051400851251402, 0.307007461046049, 0.3056770760172209, 0.5142838100007573, 0.3123713078860001, 0.4685172971176177]\n\ntrained_years=[2003, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]\n\n\n# Get unique years from the data\nyears = data['Year'].unique()\n\n# Iterate over each year\nfor year in tqdm(years):\n    if year in trained_years:\n        continue\n    print(f'training for the year {year}')\n    # Get indices for the current year\n    test_indices = data[data['Year'] == year].index\n    train_indices = data[data['Year'] != year].index\n    \n    # Create training and test sets\n    X_train, X_test = X_final[train_indices,:], X_final[test_indices,:]\n    y_train, y_test = y_final[train_indices,:], y_final[test_indices,:]\n    \n    # Train the model\n    upscaling_model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = upscaling_model.predict(X_test)\n    \n    # Calculate metrics\n    r2 = r2_score(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    print(f'R2 score : {r2}')\n    print(f'MSE : {mse}')\n    # Store results\n    r2_scores.append(r2)\n    mse_scores.append(mse)\n\n# Convert scores to arrays for easier analysis\nr2_scores = np.array(r2_scores)\nmse_scores = np.array(mse_scores)\n\n# Print results\nfor year, r2, mse in zip(years, r2_scores, mse_scores):\n    print(f\"Year: {year} - R² Score: {r2:.4f}, MSE: {mse:.4f}\")\n\n# Optionally, calculate average scores\nprint(f\"Average R² Score: {r2_scores.mean():.4f}\")\nprint(f\"Average MSE: {mse_scores.mean():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ploting leave-one-year-out results\n#### Change the R2 score with your results","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Data\nyears = [2003, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, \n         2017, 2018, 2019, 2020, 2021, 2022, 2005]\nr2_scores = [0.8856, 0.9228, 0.9083, 0.9390, 0.9549, 0.9662, 0.9657, 0.9662, 0.9513, \n             0.9701, 0.9554, 0.9620, 0.9704, 0.9511, 0.9171, 0.9264, 0.9352, 0.8805, \n             0.7149, 0.9060]\nmse_values = [0.7768, 0.5071, 0.6576, 0.4600, 0.3617, 0.3051, 0.3070, 0.3057, 0.5143, \n              0.3124, 0.4685, 0.3901, 0.3137, 0.5304, 0.9019, 0.8445, 0.7113, 1.2545, \n              3.9933, 0.6573]\n\n# Sorting the data by years for proper chronological order in the plot\nsorted_data = sorted(zip(years, r2_scores, mse_values))\nyears, r2_scores, mse_values = zip(*sorted_data)\n\n# Creating the plot\nfig, ax1 = plt.subplots(figsize=(10, 6))\n\n# Plotting R² Score on the primary y-axis\nax1.plot(years, r2_scores, color='blue', marker='o', label=\"R² Score\")\nax1.set_xlabel('Year')\nax1.set_ylabel('R² Score', color='blue')\nax1.tick_params(axis='y', labelcolor='blue')\n\n# Adding average R² Score as a horizontal line\nax1.axhline(y=0.9275, color='blue', linestyle='--', label=\"Average R² Score\")\n\n# Creating a secondary y-axis for MSE values\nax2 = ax1.twinx()\nax2.plot(years, mse_values, color='red', marker='s', label=\"MSE\")\nax2.set_ylabel('MSE', color='red')\nax2.tick_params(axis='y', labelcolor='red')\n\n# Adding average MSE as a horizontal line\nax2.axhline(y=0.7287, color='red', linestyle='--', label=\"Average MSE\")\n\n# Adding legend and title\nfig.suptitle(\"R² Score and MSE over Years\")\nax1.legend(loc=\"upper left\")\nax2.legend(loc=\"upper right\")\nplt.savefig('leave_1_year_out.png')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The lower portion is not much important please go the GroundHog: Testing and Downstream task notebook for further analysis","metadata":{}},{"cell_type":"code","source":"import joblib\n# Load the saved model\nloaded_model = joblib.load('/kaggle/input/upscaling-model-weight/upscaling_model.joblib')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Get feature importances\nimportances = loaded_model.feature_importances_.round(3)\n\n# Create a DataFrame to display feature importances\nfeature_names = encoded_columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.inspection import PartialDependenceDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_partial_dependence(\n    model, X_train, feature_names, selected_features, target_name=\"Maximum GWL\", features_per_plot=3,\n    space_for_top=0.95,file_name='PDP'\n):\n    \"\"\"\n    Plots Partial Dependence Plots for selected features based on feature importance.\n\n    Parameters:\n    - model: Trained model with `feature_importances_` attribute.\n    - X_train: Training data used for the model (numpy array or DataFrame).\n    - feature_names: List of feature names corresponding to columns in X_train.\n    - selected_features: List of feature names for which to plot partial dependence.\n    - target_name: Name of the target variable for y-axis labeling (default is \"Maximum GWL\").\n    - features_per_plot: Number of features per row in the subplot grid (default is 3).\n    \"\"\"\n\n    # Ensure feature_names length matches X_train columns\n    if len(feature_names) != X_train.shape[1]:\n        raise ValueError(\"The number of feature names must match the number of features in X_train.\")\n\n    # Create a dictionary to map each feature to its index for quick lookup\n    feature_index_map = {name: idx for idx, name in enumerate(feature_names)}\n\n    # Filter selected features by their indexes\n    selected_indexes = [feature_index_map[feat] for feat in selected_features if feat in feature_index_map]\n\n    # Get importances and sort selected indexes by importance\n    importances = model.feature_importances_\n    selected_indexes = sorted(selected_indexes, key=lambda x: importances[x], reverse=True)\n    \n    # Calculate the number of rows for subplots based on features_per_plot\n    num_plots = np.ceil(len(selected_indexes) / features_per_plot).astype(int)\n    \n    # Set up the figure with subplots\n    fig, axes = plt.subplots(num_plots, features_per_plot, figsize=(15, num_plots * 5))\n    axes = axes.flatten()  # Flatten for easy indexing\n\n    # Plot each selected feature\n    for i, feature_index in enumerate(selected_indexes):\n        PartialDependenceDisplay.from_estimator(\n            model, X_train, [feature_index], feature_names=feature_names, ax=axes[i]\n        )\n        axes[i].set_title(feature_names[feature_index])  # Set title for each subplot\n        axes[i].set_ylabel(target_name)  # Set y-axis label\n\n        # Customize x-ticks for clarity\n        x_ticks = axes[i].get_xticks()\n        axes[i].set_xticks(np.linspace(x_ticks[0], x_ticks[-1], num=10))\n        axes[i].set_xticklabels(np.round(np.linspace(x_ticks[0], x_ticks[-1], num=10), 2))\n\n    # Hide any unused subplots\n    for j in range(len(selected_indexes), num_plots * features_per_plot):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.suptitle(f'Partial Dependence ({target_name}) Plots by Feature \\n (change of {target_name} if only that Feature is changed)', fontsize=16)\n    plt.subplots_adjust(top=space_for_top)\n    plt.savefig(f'{file_name}.png')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_partial_dependence(\n#     model=loaded_model,\n#     X_train=X_test,\n#     feature_names=feature_names,\n#     selected_features=['elevation_x'],  # selected features\n#     target_name=\"Maximum GWL\",\n#     space_for_top=0.97,\n#     file_name='All_max_gwl_feature_pdp'\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sum_lithology_importance(importance_df):\n    \"\"\"\n    Sums the importances of all features that match the pattern 'lithology_X' (excluding 'lithology_clay_thickness'),\n    and replaces them with a new row 'lithology' in the importance DataFrame.\n\n    Args:\n        importance_df (pd.DataFrame): DataFrame with 'Feature' and 'Importance' columns.\n\n    Returns:\n        pd.DataFrame: Modified DataFrame with summed 'lithology' importance.\n    \"\"\"\n    # Identify rows where the 'Feature' column matches 'lithology_X', excluding 'lithology_clay_thickness'\n    lithology_rows = (importance_df['Feature'].str.startswith('lithology_') & \n                      ~importance_df['Feature'].str.contains('lithology_clay_thickness'))\n    \n    # Sum the 'Importance' values for the identified lithology features\n    lithology_importance_sum = importance_df[lithology_rows]['Importance'].sum()\n\n    # Drop the original lithology_X rows\n    importance_df = importance_df[~lithology_rows]\n    \n    # Add a new row for the summed 'lithology' importance\n    new_row = pd.DataFrame({'Feature': ['lithology'], 'Importance': [lithology_importance_sum]})\n    print(\"new row:\",new_row)\n    importance_df = pd.concat([importance_df, new_row], ignore_index=True)\n\n    # Sort the DataFrame by 'Importance' in descending order\n    importance_df = importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n    \n    return importance_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\nimportance_df = sum_lithology_importance(pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': importances\n}).sort_values(by='Importance', ascending=False))\n\n# Dis","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Display the feature importances\nprint(importance_df)\n\n# Optionally, plot the feature importances\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\nplt.xlabel('Importance')\nplt.title('Feature Importances from Upscaling Random Forest Regressor')\nplt.gca().invert_yaxis()  # To display the most important features at the top\nplt.savefig('Upscaling_model_feature_importance.png', dpi=300, bbox_inches='tight')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loaded_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_all=loaded_model.predict(X_all)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['Upscaled min']=y_pred_all[:,0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['Upscaled max']=y_pred_all[:,1]","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}